%!
%%BoundingBox: (atend)
%%Pages: (atend)
%%DocumentFonts: (atend)
%%EndComments
%
% FrameMaker PostScript Prolog 3.0, for use with FrameMaker 3.0
% Copyright (c) 1986,87,89,90,91 by Frame Technology Corporation.
% All rights reserved.
%
% Known Problems:
%	Due to bugs in Transcript, the 'PS-Adobe-' is omitted from line 1
/FMversion (3.0) def 
% Set up Color vs. Black-and-White
	/FMPrintInColor systemdict /colorimage known
		systemdict /currentcolortransfer known or def
% Uncomment this line to force b&w on color printer
%   /FMPrintInColor false def
/FrameDict 195 dict def 
systemdict /errordict known not {/errordict 10 dict def
		errordict /rangecheck {stop} put} if
% The readline in 23.0 doesn't recognize cr's as nl's on AppleTalk
FrameDict /tmprangecheck errordict /rangecheck get put 
errordict /rangecheck {FrameDict /bug true put} put 
FrameDict /bug false put 
mark 
% Some PS machines read past the CR, so keep the following 3 lines together!
currentfile 5 string readline
00
0000000000
cleartomark 
errordict /rangecheck FrameDict /tmprangecheck get put 
FrameDict /bug get { 
	/readline {
		/gstring exch def
		/gfile exch def
		/gindex 0 def
		{
			gfile read pop 
			dup 10 eq {exit} if 
			dup 13 eq {exit} if 
			gstring exch gindex exch put 
			/gindex gindex 1 add def 
		} loop
		pop 
		gstring 0 gindex getinterval true 
		} def
	} if
/FMVERSION {
	FMversion ne {
		/Times-Roman findfont 18 scalefont setfont
		100 100 moveto
		(FrameMaker version does not match postscript_prolog!)
		dup =
		show showpage
		} if
	} def 
/FMLOCAL {
	FrameDict begin
	0 def 
	end 
	} def 
	/gstring FMLOCAL
	/gfile FMLOCAL
	/gindex FMLOCAL
	/orgxfer FMLOCAL
	/orgproc FMLOCAL
	/organgle FMLOCAL
	/orgfreq FMLOCAL
	/yscale FMLOCAL
	/xscale FMLOCAL
	/manualfeed FMLOCAL
	/paperheight FMLOCAL
	/paperwidth FMLOCAL
/FMDOCUMENT { 
	array /FMfonts exch def 
	/#copies exch def
	FrameDict begin
	0 ne dup {setmanualfeed} if
	/manualfeed exch def
	/paperheight exch def
	/paperwidth exch def
	/yscale exch def
	/xscale exch def
	currenttransfer cvlit /orgxfer exch def
	currentscreen cvlit /orgproc exch def
	/organgle exch def /orgfreq exch def
	setpapername 
	manualfeed {true} {papersize} ifelse 
	{manualpapersize} {false} ifelse 
	{desperatepapersize} if
	end 
	} def 
	/pagesave FMLOCAL
	/orgmatrix FMLOCAL
	/landscape FMLOCAL
/FMBEGINPAGE { 
	FrameDict begin 
	/pagesave save def
	3.86 setmiterlimit
	/landscape exch 0 ne def
	landscape { 
		90 rotate 0 exch neg translate pop 
		}
		{pop pop}
		ifelse
	xscale yscale scale
	/orgmatrix matrix def
	gsave 
	} def 
/FMENDPAGE {
	grestore 
	pagesave restore
	end 
	showpage
	} def 
/FMFONTDEFINE { 
	FrameDict begin
	findfont 
	ReEncode 
	1 index exch 
	definefont 
	FMfonts 3 1 roll 
	put
	end 
	} def 
/FMFILLS {
	FrameDict begin
	array /fillvals exch def
	end 
	} def 
/FMFILL {
	FrameDict begin
	 fillvals 3 1 roll put
	end 
	} def 
/FMNORMALIZEGRAPHICS { 
	newpath
	0.0 0.0 moveto
	1 setlinewidth
	0 setlinecap
	0 0 0 sethsbcolor
	0 setgray 
	} bind def
	/fx FMLOCAL
	/fy FMLOCAL
	/fh FMLOCAL
	/fw FMLOCAL
	/llx FMLOCAL
	/lly FMLOCAL
	/urx FMLOCAL
	/ury FMLOCAL
/FMBEGINEPSF { 
	end 
	/FMEPSF save def 
	/showpage {} def 
	FMNORMALIZEGRAPHICS 
	[/fy /fx /fh /fw /ury /urx /lly /llx] {exch def} forall 
	fx fy translate 
	rotate
	fw urx llx sub div fh ury lly sub div scale 
	llx neg lly neg translate 
	} bind def
/FMENDEPSF {
	FMEPSF restore
	FrameDict begin 
	} bind def
FrameDict begin 
/setmanualfeed {
%%BeginFeature *ManualFeed True
	 statusdict /manualfeed true put
%%EndFeature
	} def
/max {2 copy lt {exch} if pop} bind def
/min {2 copy gt {exch} if pop} bind def
/inch {72 mul} def
/pagedimen { 
	paperheight sub abs 16 lt exch 
	paperwidth sub abs 16 lt and
	{/papername exch def} {pop} ifelse
	} def
	/papersizedict FMLOCAL
/setpapername { 
	/papersizedict 14 dict def 
	papersizedict begin
	/papername /unknown def 
		/Letter 8.5 inch 11.0 inch pagedimen
		/LetterSmall 7.68 inch 10.16 inch pagedimen
		/Tabloid 11.0 inch 17.0 inch pagedimen
		/Ledger 17.0 inch 11.0 inch pagedimen
		/Legal 8.5 inch 14.0 inch pagedimen
		/Statement 5.5 inch 8.5 inch pagedimen
		/Executive 7.5 inch 10.0 inch pagedimen
		/A3 11.69 inch 16.5 inch pagedimen
		/A4 8.26 inch 11.69 inch pagedimen
		/A4Small 7.47 inch 10.85 inch pagedimen
		/B4 10.125 inch 14.33 inch pagedimen
		/B5 7.16 inch 10.125 inch pagedimen
	end
	} def
/papersize {
	papersizedict begin
		/Letter {lettertray letter} def
		/LetterSmall {lettertray lettersmall} def
		/Tabloid {11x17tray 11x17} def
		/Ledger {ledgertray ledger} def
		/Legal {legaltray legal} def
		/Statement {statementtray statement} def
		/Executive {executivetray executive} def
		/A3 {a3tray a3} def
		/A4 {a4tray a4} def
		/A4Small {a4tray a4small} def
		/B4 {b4tray b4} def
		/B5 {b5tray b5} def
		/unknown {unknown} def
	papersizedict dup papername known {papername} {/unknown} ifelse get
	end
	/FMdicttop countdictstack 1 add def 
	statusdict begin stopped end 
	countdictstack -1 FMdicttop {pop end} for 
	} def
/manualpapersize {
	papersizedict begin
		/Letter {letter} def
		/LetterSmall {lettersmall} def
		/Tabloid {11x17} def
		/Ledger {ledger} def
		/Legal {legal} def
		/Statement {statement} def
		/Executive {executive} def
		/A3 {a3} def
		/A4 {a4} def
		/A4Small {a4small} def
		/B4 {b4} def
		/B5 {b5} def
		/unknown {unknown} def
	papersizedict dup papername known {papername} {/unknown} ifelse get
	end
	stopped 
	} def
/desperatepapersize {
	statusdict /setpageparams known
		{
		paperwidth paperheight 0 1 
		statusdict begin
		{setpageparams} stopped pop 
		end
		} if
	} def
/savematrix {
	orgmatrix currentmatrix pop
	} bind def
/restorematrix {
	orgmatrix setmatrix
	} bind def
/dmatrix matrix def
/dpi    72 0 dmatrix defaultmatrix dtransform
    dup mul exch   dup mul add   sqrt def
/freq dpi 18.75 div 8 div round dup 0 eq {pop 1} if 8 mul dpi exch div def
/sangle 1 0 dmatrix defaultmatrix dtransform exch atan def
/DiacriticEncoding [
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /space /exclam /quotedbl
/numbersign /dollar /percent /ampersand /quotesingle /parenleft
/parenright /asterisk /plus /comma /hyphen /period /slash /zero /one
/two /three /four /five /six /seven /eight /nine /colon /semicolon
/less /equal /greater /question /at /A /B /C /D /E /F /G /H /I /J /K
/L /M /N /O /P /Q /R /S /T /U /V /W /X /Y /Z /bracketleft /backslash
/bracketright /asciicircum /underscore /grave /a /b /c /d /e /f /g /h
/i /j /k /l /m /n /o /p /q /r /s /t /u /v /w /x /y /z /braceleft /bar
/braceright /asciitilde /.notdef /Adieresis /Aring /Ccedilla /Eacute
/Ntilde /Odieresis /Udieresis /aacute /agrave /acircumflex /adieresis
/atilde /aring /ccedilla /eacute /egrave /ecircumflex /edieresis
/iacute /igrave /icircumflex /idieresis /ntilde /oacute /ograve
/ocircumflex /odieresis /otilde /uacute /ugrave /ucircumflex
/udieresis /dagger /.notdef /cent /sterling /section /bullet
/paragraph /germandbls /registered /copyright /trademark /acute
/dieresis /.notdef /AE /Oslash /.notdef /.notdef /.notdef /.notdef
/yen /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/ordfeminine /ordmasculine /.notdef /ae /oslash /questiondown
/exclamdown /logicalnot /.notdef /florin /.notdef /.notdef
/guillemotleft /guillemotright /ellipsis /.notdef /Agrave /Atilde
/Otilde /OE /oe /endash /emdash /quotedblleft /quotedblright
/quoteleft /quoteright /.notdef /.notdef /ydieresis /Ydieresis
/fraction /currency /guilsinglleft /guilsinglright /fi /fl /daggerdbl
/periodcentered /quotesinglbase /quotedblbase /perthousand
/Acircumflex /Ecircumflex /Aacute /Edieresis /Egrave /Iacute
/Icircumflex /Idieresis /Igrave /Oacute /Ocircumflex /.notdef /Ograve
/Uacute /Ucircumflex /Ugrave /dotlessi /circumflex /tilde /macron
/breve /dotaccent /ring /cedilla /hungarumlaut /ogonek /caron
] def
/ReEncode { 
	dup 
	length 
	dict begin 
	{
	1 index /FID ne 
		{def} 
		{pop pop} ifelse 
	} forall 
	0 eq {/Encoding DiacriticEncoding def} if 
	currentdict 
	end 
	} bind def
/graymode true def
	/bwidth FMLOCAL
	/bpside FMLOCAL
	/bstring FMLOCAL
	/onbits FMLOCAL
	/offbits FMLOCAL
	/xindex FMLOCAL
	/yindex FMLOCAL
	/x FMLOCAL
	/y FMLOCAL
/setpattern {
	 /bwidth  exch def
	 /bpside  exch def
	 /bstring exch def
	 /onbits 0 def  /offbits 0 def
	 freq sangle landscape {90 add} if 
		{/y exch def
		 /x exch def
		 /xindex x 1 add 2 div bpside mul cvi def
		 /yindex y 1 add 2 div bpside mul cvi def
		 bstring yindex bwidth mul xindex 8 idiv add get
		 1 7 xindex 8 mod sub bitshift and 0 ne
		 {/onbits  onbits  1 add def 1}
		 {/offbits offbits 1 add def 0}
		 ifelse
		}
		setscreen
	 {} settransfer
	 offbits offbits onbits add div FMsetgray
	/graymode false def
	} bind def
/grayness {
	FMsetgray
	graymode not {
		/graymode true def
		orgxfer cvx settransfer
		orgfreq organgle orgproc cvx setscreen
		} if
	} bind def
	/HUE FMLOCAL
	/SAT FMLOCAL
	/BRIGHT FMLOCAL
	/Colors FMLOCAL
FMPrintInColor 
	
	{
	/HUE 0 def
	/SAT 0 def
	/BRIGHT 0 def
	% array of arrays Hue and Sat values for the separations [HUE BRIGHT]
	/Colors   
	[[0    0  ]    % black
	 [0    0  ]    % white
	 [0.00 1.0]    % red
	 [0.37 1.0]    % green
	 [0.60 1.0]    % blue
	 [0.50 1.0]    % cyan
	 [0.83 1.0]    % magenta
	 [0.16 1.0]    % comment / yellow
	 ] def
      
	/BEGINBITMAPCOLOR { 
		BITMAPCOLOR} def
	/BEGINBITMAPCOLORc { 
		BITMAPCOLORc} def
	/BEGINBITMAPTRUECOLOR { 
		BITMAPTRUECOLOR } def
	/BEGINBITMAPTRUECOLORc { 
		BITMAPTRUECOLORc } def
	/K { 
		Colors exch get dup
		0 get /HUE exch store 
		1 get /BRIGHT exch store
		  HUE 0 eq BRIGHT 0 eq and
			{1.0 SAT sub setgray}
			{HUE SAT BRIGHT sethsbcolor} 
		  ifelse
		} def
	/FMsetgray { 
		/SAT exch 1.0 exch sub store 
		  HUE 0 eq BRIGHT 0 eq and
			{1.0 SAT sub setgray}
			{HUE SAT BRIGHT sethsbcolor} 
		  ifelse
		} bind def
	}
	
	{
	/BEGINBITMAPCOLOR { 
		BITMAPGRAY} def
	/BEGINBITMAPCOLORc { 
		BITMAPGRAYc} def
	/BEGINBITMAPTRUECOLOR { 
		BITMAPTRUEGRAY } def
	/BEGINBITMAPTRUECOLORc { 
		BITMAPTRUEGRAYc } def
	/FMsetgray {setgray} bind def
	/K { 
		pop
		} def
	}
ifelse
/normalize {
	transform round exch round exch itransform
	} bind def
/dnormalize {
	dtransform round exch round exch idtransform
	} bind def
/lnormalize { 
	0 dtransform exch cvi 2 idiv 2 mul 1 add exch idtransform pop
	} bind def
/H { 
	lnormalize setlinewidth
	} bind def
/Z {
	setlinecap
	} bind def
	/fillvals FMLOCAL
/X { 
	fillvals exch get
	dup type /stringtype eq
	{8 1 setpattern} 
	{grayness}
	ifelse
	} bind def
/V { 
	gsave eofill grestore
	} bind def
/N { 
	stroke
	} bind def
/M {newpath moveto} bind def
/E {lineto} bind def
/D {curveto} bind def
/O {closepath} bind def
	/n FMLOCAL
/L { 
 	/n exch def
	newpath
	normalize
	moveto 
	2 1 n {pop normalize lineto} for
	} bind def
/Y { 
	L 
	closepath
	} bind def
	/x1 FMLOCAL
	/x2 FMLOCAL
	/y1 FMLOCAL
	/y2 FMLOCAL
	/rad FMLOCAL
/R { 
	/y2 exch def
	/x2 exch def
	/y1 exch def
	/x1 exch def
	x1 y1
	x2 y1
	x2 y2
	x1 y2
	4 Y 
	} bind def
/RR { 
	/rad exch def
	normalize
	/y2 exch def
	/x2 exch def
	normalize
	/y1 exch def
	/x1 exch def
	newpath
	x1 y1 rad add moveto
	x1 y2 x2 y2 rad arcto
	x2 y2 x2 y1 rad arcto
	x2 y1 x1 y1 rad arcto
	x1 y1 x1 y2 rad arcto
	closepath
	16 {pop} repeat
	} bind def
/C { 
	grestore
	gsave
	R 
	clip
	} bind def
	/FMpointsize FMLOCAL
/F { 
	FMfonts exch get
	FMpointsize scalefont
	setfont
	} bind def
/Q { 
	/FMpointsize exch def
	F 
	} bind def
/T { 
	moveto show
	} bind def
/RF { 
	rotate
	0 ne {-1 1 scale} if
	} bind def
/TF { 
	gsave
	moveto 
	RF
	show
	grestore
	} bind def
/P { 
	moveto
	0 32 3 2 roll widthshow
	} bind def
/PF { 
	gsave
	moveto 
	RF
	0 32 3 2 roll widthshow
	grestore
	} bind def
/S { 
	moveto
	0 exch ashow
	} bind def
/SF { 
	gsave
	moveto
	RF
	0 exch ashow
	grestore
	} bind def
/B { 
	moveto
	0 32 4 2 roll 0 exch awidthshow
	} bind def
/BF { 
	gsave
	moveto
	RF
	0 32 4 2 roll 0 exch awidthshow
	grestore
	} bind def
/G { 
	gsave
	newpath
	normalize translate 0.0 0.0 moveto 
	dnormalize scale 
	0.0 0.0 1.0 5 3 roll arc 
	closepath fill
	grestore
	} bind def
/A { 
	gsave
	savematrix
	newpath
	2 index 2 div add exch 3 index 2 div sub exch 
	normalize 2 index 2 div sub exch 3 index 2 div add exch 
	translate 
	scale 
	0.0 0.0 1.0 5 3 roll arc 
	restorematrix
	stroke
	grestore
	} bind def
	/x FMLOCAL
	/y FMLOCAL
	/w FMLOCAL
	/h FMLOCAL
	/xx FMLOCAL
	/yy FMLOCAL
	/ww FMLOCAL
	/hh FMLOCAL
	/FMsaveobject FMLOCAL
	/FMoptop FMLOCAL
	/FMdicttop FMLOCAL
/BEGINPRINTCODE { 
	/FMdicttop countdictstack 1 add def 
	/FMoptop count 4 sub def 
	/FMsaveobject save def
	userdict begin 
	/showpage {} def 
	FMNORMALIZEGRAPHICS 
	3 index neg 3 index neg translate
	} bind def
/ENDPRINTCODE {
	count -1 FMoptop {pop pop} for 
	countdictstack -1 FMdicttop {pop end} for 
	FMsaveobject restore 
	} bind def
/gn { 
	0 
	{	46 mul 
		cf read pop 
		32 sub 
		dup 46 lt {exit} if 
		46 sub add 
		} loop
	add 
	} bind def
	/str FMLOCAL
/cfs { 
	/str sl string def 
	0 1 sl 1 sub {str exch val put} for 
	str def 
	} bind def
/ic [ 
	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0223
	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0223
	0
	{0 hx} {1 hx} {2 hx} {3 hx} {4 hx} {5 hx} {6 hx} {7 hx} {8 hx} {9 hx}
	{10 hx} {11 hx} {12 hx} {13 hx} {14 hx} {15 hx} {16 hx} {17 hx} {18 hx}
	{19 hx} {gn hx} {0} {1} {2} {3} {4} {5} {6} {7} {8} {9} {10} {11} {12}
	{13} {14} {15} {16} {17} {18} {19} {gn} {0 wh} {1 wh} {2 wh} {3 wh}
	{4 wh} {5 wh} {6 wh} {7 wh} {8 wh} {9 wh} {10 wh} {11 wh} {12 wh}
	{13 wh} {14 wh} {gn wh} {0 bl} {1 bl} {2 bl} {3 bl} {4 bl} {5 bl} {6 bl}
	{7 bl} {8 bl} {9 bl} {10 bl} {11 bl} {12 bl} {13 bl} {14 bl} {gn bl}
	{0 fl} {1 fl} {2 fl} {3 fl} {4 fl} {5 fl} {6 fl} {7 fl} {8 fl} {9 fl}
	{10 fl} {11 fl} {12 fl} {13 fl} {14 fl} {gn fl}
	] def
	/sl FMLOCAL
	/val FMLOCAL
	/ws FMLOCAL
	/im FMLOCAL
	/bs FMLOCAL
	/cs FMLOCAL
	/len FMLOCAL
	/pos FMLOCAL
/ms { 
	/sl exch def 
	/val 255 def 
	/ws cfs 
	/im cfs 
	/val 0 def 
	/bs cfs 
	/cs cfs 
	} bind def
400 ms 
/ip { 
	is 
	0 
	cf cs readline pop 
	{	ic exch get exec 
		add 
		} forall 
	pop 
	
	} bind def
/wh { 
	/len exch def 
	/pos exch def 
	ws 0 len getinterval im pos len getinterval copy pop
	pos len 
	} bind def
/bl { 
	/len exch def 
	/pos exch def 
	bs 0 len getinterval im pos len getinterval copy pop
	pos len 
	} bind def
/s1 1 string def
/fl { 
	/len exch def 
	/pos exch def 
	/val cf s1 readhexstring pop 0 get def
	pos 1 pos len add 1 sub {im exch val put} for
	pos len 
	} bind def
/hx { 
	3 copy getinterval 
	cf exch readhexstring pop pop 
	} bind def
	/h FMLOCAL
	/w FMLOCAL
	/d FMLOCAL
	/lb FMLOCAL
	/bitmapsave FMLOCAL
	/is FMLOCAL
	/cf FMLOCAL
/wbytes { 
	dup 
	8 eq {pop} {1 eq {7 add 8 idiv} {3 add 4 idiv} ifelse} ifelse
	} bind def
/BEGINBITMAPBWc { 
	1 {} COMMONBITMAPc
	} bind def
/BEGINBITMAPGRAYc { 
	8 {} COMMONBITMAPc
	} bind def
/BEGINBITMAP2BITc { 
	2 {} COMMONBITMAPc
	} bind def
/COMMONBITMAPc { 
	/r exch def
	/d exch def
	gsave
	translate rotate scale /h exch def /w exch def
	/lb w d wbytes def 
	sl lb lt {lb ms} if 
	/bitmapsave save def 
	r                    
	/is im 0 lb getinterval def 
	ws 0 lb getinterval is copy pop 
	/cf currentfile def 
	w h d [w 0 0 h neg 0 h] 
	{ip} image 
	bitmapsave restore 
	grestore
	} bind def
/BEGINBITMAPBW { 
	1 {} COMMONBITMAP
	} bind def
/BEGINBITMAPGRAY { 
	8 {} COMMONBITMAP
	} bind def
/BEGINBITMAP2BIT { 
	2 {} COMMONBITMAP
	} bind def
/COMMONBITMAP { 
	/r exch def
	/d exch def
	gsave
	translate rotate scale /h exch def /w exch def
	/bitmapsave save def 
	r                    
	/is w d wbytes string def
	/cf currentfile def 
	w h d [w 0 0 h neg 0 h] 
	{cf is readhexstring pop} image
	bitmapsave restore 
	grestore
	} bind def
	/proc1 FMLOCAL
	/proc2 FMLOCAL
	/newproc FMLOCAL
/Fmcc {
    /proc2 exch cvlit def
    /proc1 exch cvlit def
    /newproc proc1 length proc2 length add array def
    newproc 0 proc1 putinterval
    newproc proc1 length proc2 putinterval
    newproc cvx
} bind def
/ngrayt 256 array def
/nredt 256 array def
/nbluet 256 array def
/ngreent 256 array def
	/gryt FMLOCAL
	/blut FMLOCAL
	/grnt FMLOCAL
	/redt FMLOCAL
	/indx FMLOCAL
	/cynu FMLOCAL
	/magu FMLOCAL
	/yelu FMLOCAL
	/k FMLOCAL
	/u FMLOCAL
/colorsetup {
	currentcolortransfer
	/gryt exch def
	/blut exch def
	/grnt exch def
	/redt exch def
	0 1 255 {
		/indx exch def
		/cynu 1 red indx get 255 div sub def
		/magu 1 green indx get 255 div sub def
		/yelu 1 blue indx get 255 div sub def
		/k cynu magu min yelu min def
		/u k currentundercolorremoval exec def
		nredt indx 1 0 cynu u sub max sub redt exec put
		ngreent indx 1 0 magu u sub max sub grnt exec put
		nbluet indx 1 0 yelu u sub max sub blut exec put
		ngrayt indx 1 k currentblackgeneration exec sub gryt exec put
	} for
	{255 mul cvi nredt exch get}
	{255 mul cvi ngreent exch get}
	{255 mul cvi nbluet exch get}
	{255 mul cvi ngrayt exch get}
	setcolortransfer
	{pop 0} setundercolorremoval
	{} setblackgeneration
	} bind def
	/tran FMLOCAL
/fakecolorsetup {
	/tran 256 string def
	0 1 255 {/indx exch def 
		tran indx
		red indx get 77 mul
		green indx get 151 mul
		blue indx get 28 mul
		add add 256 idiv put} for
	currenttransfer
	{255 mul cvi tran exch get 255.0 div}
	exch Fmcc settransfer
} bind def
/BITMAPCOLOR { 
	/d 8 def
	gsave
	translate rotate scale /h exch def /w exch def
	/bitmapsave save def 
	colorsetup
	/is w d wbytes string def
	/cf currentfile def 
	w h d [w 0 0 h neg 0 h] 
	{cf is readhexstring pop} {is} {is} true 3 colorimage 
	bitmapsave restore 
	grestore
	} bind def
/BITMAPCOLORc { 
	/d 8 def
	gsave
	translate rotate scale /h exch def /w exch def
	/lb w d wbytes def 
	sl lb lt {lb ms} if 
	/bitmapsave save def 
	colorsetup
	/is im 0 lb getinterval def 
	ws 0 lb getinterval is copy pop 
	/cf currentfile def 
	w h d [w 0 0 h neg 0 h] 
	{ip} {is} {is} true 3 colorimage
	bitmapsave restore 
	grestore
	} bind def
/BITMAPTRUECOLORc { 
        gsave
        translate rotate scale /h exch def /w exch def
        /bitmapsave save def 
        
        /is w string def
        
        ws 0 w getinterval is copy pop 
        /cf currentfile def 
        w h 8 [w 0 0 h neg 0 h] 
        {ip} {gip} {bip} true 3 colorimage
        bitmapsave restore 
        grestore
        } bind def
/BITMAPTRUECOLOR { 
        gsave
        translate rotate scale /h exch def /w exch def
        /bitmapsave save def 
        /is w string def
        /gis w string def
        /bis w string def
        /cf currentfile def 
        w h 8 [w 0 0 h neg 0 h] 
        { cf is readhexstring pop } 
        { cf gis readhexstring pop } 
        { cf bis readhexstring pop } 
        true 3 colorimage 
        bitmapsave restore 
        grestore
        } bind def
/BITMAPTRUEGRAYc { 
        gsave
        translate rotate scale /h exch def /w exch def
        /bitmapsave save def 
        
        /is w string def
        
        ws 0 w getinterval is copy pop 
        /cf currentfile def 
        w h 8 [w 0 0 h neg 0 h] 
        {ip gip bip w gray} image
        bitmapsave restore 
        grestore
        } bind def
/ww FMLOCAL
/r FMLOCAL
/g FMLOCAL
/b FMLOCAL
/i FMLOCAL
/gray { 
        /ww exch def
        /b exch def
        /g exch def
        /r exch def
        0 1 ww 1 sub { /i exch def r i get .299 mul g i get .587 mul
			b i get .114 mul add add r i 3 -1 roll floor cvi put } for
        r
        } bind def
/BITMAPTRUEGRAY { 
        gsave
        translate rotate scale /h exch def /w exch def
        /bitmapsave save def 
        /is w string def
        /gis w string def
        /bis w string def
        /cf currentfile def 
        w h 8 [w 0 0 h neg 0 h] 
        { cf is readhexstring pop 
          cf gis readhexstring pop 
          cf bis readhexstring pop w gray}  image
        bitmapsave restore 
        grestore
        } bind def
/BITMAPGRAY { 
	8 {fakecolorsetup} COMMONBITMAP
	} bind def
/BITMAPGRAYc { 
	8 {fakecolorsetup} COMMONBITMAPc
	} bind def
/ENDBITMAP {
	} bind def
end 
	/ALDsave FMLOCAL
	/ALDmatrix matrix def ALDmatrix currentmatrix pop
/StartALD {
	/ALDsave save def
	 savematrix
	 ALDmatrix setmatrix
	} bind def
/InALD {
	 restorematrix
	} bind def
/DoneALD {
	 ALDsave restore
	} bind def
%%EndProlog
%%BeginSetup
(3.0) FMVERSION
1 1 612 792 0 1 12 FMDOCUMENT
0 0 /Times-Roman FMFONTDEFINE
1 0 /Times-Italic FMFONTDEFINE
2 0 /Times-Bold FMFONTDEFINE
3 0 /Courier-Bold FMFONTDEFINE
32 FMFILLS
0 0 FMFILL
1 .1 FMFILL
2 .3 FMFILL
3 .5 FMFILL
4 .7 FMFILL
5 .9 FMFILL
6 .97 FMFILL
7 1 FMFILL
8 <0f1e3c78f0e1c387> FMFILL
9 <0f87c3e1f0783c1e> FMFILL
10 <cccccccccccccccc> FMFILL
11 <ffff0000ffff0000> FMFILL
12 <8142241818244281> FMFILL
13 <03060c183060c081> FMFILL
14 <8040201008040201> FMFILL
16 1 FMFILL
17 .9 FMFILL
18 .7 FMFILL
19 .5 FMFILL
20 .3 FMFILL
21 .1 FMFILL
22 0.03 FMFILL
23 0 FMFILL
24 <f0e1c3870f1e3c78> FMFILL
25 <f0783c1e0f87c3e1> FMFILL
26 <3333333333333333> FMFILL
27 <0000ffff0000ffff> FMFILL
28 <7ebddbe7e7dbbd7e> FMFILL
29 <fcf9f3e7cf9f3f7e> FMFILL
30 <7fbfdfeff7fbfdfe> FMFILL
%%EndSetup
%%Page: "11" 11
%%BeginPaperSize: Letter
%%EndPaperSize
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
0 10 Q
([Foglesong90]) 72 713.33 T
6.91 (Joy Foglesong, George Richmond,) 137.2 713.33 P
3.17 (Loellyn Cassell, Carole Hogan, John) 137.2 701.33 P
4.28 (Kordas, and Michael Nemanic. The) 137.2 689.33 P
0.8 (Livermore Distributed Storage System:) 137.2 677.33 P
0.71 (Implementation and Experiences.) 137.2 665.33 P
1 F
0.71 (Mass) 275.9 665.33 P
(Storage Symposium) 137.2 653.33 T
0 F
(, May 1990.) 216.31 653.33 T
([Hennessy91]) 72 635.33 T
8.58 (John) 137.2 635.33 P
8.58 (L. Hennessy and Norman) 158.57 635.33 P
8.58 (P.) 288.94 635.33 P
7 (Jouppi. Computer Technology and) 137.2 623.33 P
1.64 (Architecture: An Evolving Interaction.) 137.2 611.33 P
1 F
12.65 (IEEE Computer) 137.2 599.33 P
0 F
12.65 (, pages 18\32029,) 213.97 599.33 P
(September 1991.) 137.2 587.33 T
([Horton90]) 72 569.33 T
1.85 (William) 137.2 569.33 P
1.85 (A. Horton and Bruce Nelson.) 172.46 569.33 P
3.22 (The Auspex NS 5000 and the SUN) 137.2 557.33 P
3.81 (SPARCserver 490 in One and Two) 137.2 545.33 P
25.78 (Ethernet NFS Performance) 137.2 533.33 P
-0.03 (Comparisons. Technical Report Auspex) 137.2 521.33 P
3.17 (Performance Report 2, Auspex, May) 137.2 509.33 P
(1990.) 137.2 497.33 T
([Katz93]) 72 479.33 T
0.95 (Randy) 137.2 479.33 P
0.95 (H. Katz, Peter) 165.79 479.33 P
0.95 (M. Chen, Ann) 226.81 479.33 P
0.95 (L.) 288.39 479.33 P
3.8 (Drapeau, Edward) 137.2 467.33 P
3.8 (K. Lee, Ken Lutz,) 213.43 467.33 P
0.12 (Ethan) 137.2 455.33 P
0.12 (L. Miller, Srinivasan Seshan, and) 163.01 455.33 P
3.49 (David) 137.2 443.33 P
3.49 (A. Patterson. RAID-II: Design) 164.12 443.33 P
2.37 (and Implementation of a Large Scale) 137.2 431.33 P
-0.37 (Disk Array Controller.) 137.2 419.33 P
1 F
-0.37 (1993 Symposium) 229.36 419.33 P
-0.16 (on Integrated Systems) 137.2 407.33 P
0 F
-0.16 (, 1993. University) 225.14 407.33 P
3.72 (of California at Berkeley UCB/CSD) 137.2 395.33 P
(92/705.) 137.2 383.33 T
([Lee91]) 72 365.33 T
4.36 (Edward) 137.2 365.33 P
4.36 (K. Lee and Randy) 170.78 365.33 P
4.36 (H. Katz.) 259.06 365.33 P
3.95 (Performance Consequences of Parity) 137.2 353.33 P
10.87 (Placement in Disk Arrays. In) 137.2 341.33 P
1 F
3.58 (Proceedings of the 4th International) 137.2 329.33 P
3.76 (Conference on Architectural Support) 137.2 317.33 P
8.57 (for Programming Languages and) 137.2 305.33 P
-0.03 (Operating Systems \050ASPLOS-IV\051) 137.2 293.33 P
0 F
-0.03 (, pages) 269.27 293.33 P
(190\320199, April 1991.) 137.2 281.33 T
([Lee92]) 72 263.33 T
-0.02 (Edward) 137.2 263.33 P
-0.02 (K. Lee, Peter) 170.78 263.33 P
-0.02 (M. Chen, John) 225.97 263.33 P
-0.02 (H.) 287.29 263.33 P
2.29 (Hartman, Ann L.) 137.2 251.33 P
2.29 (Chervenak Drapeau,) 212.56 251.33 P
10.24 (Ethan) 137.2 239.33 P
10.24 (L. Miller, Randy) 163.01 239.33 P
10.24 (H. Katz,) 253.18 239.33 P
11.63 (Garth) 137.2 227.33 P
11.63 (A. Gibson, and David) 162.45 227.33 P
11.63 (A.) 287.29 227.33 P
0.33 (Patterson. RAID-II: A Scalable Storage) 137.2 215.33 P
13.01 (Architecture for High-Bandwidth) 137.2 203.33 P
0.05 (Network File Service. Technical Report) 137.2 191.33 P
9.59 (UCB/CSD 92/672, University of) 137.2 179.33 P
(California at Berkeley, February 1992.) 137.2 167.33 T
([Nelson87]) 72 149.33 T
2.34 (Marc Nelson, David) 137.2 149.33 P
2.34 (L. Kitts, John) 225.69 149.33 P
2.34 (H.) 287.29 149.33 P
1.15 (Merrill, and Gene Harano. The NCAR) 137.2 137.33 P
-0.23 (Mass Storage System.) 137.2 125.33 P
1 F
-0.23 (IEEE Symposium) 227.56 125.33 P
8.72 (on Mass Storage) 137.2 113.33 P
0 F
8.72 (, pages 12\32020,) 221.82 113.33 P
(November 1987.) 137.2 101.33 T
([Nelson90]) 72 83.33 T
8.8 (Bruce Nelson. An Overview of) 137.2 83.33 P
4.87 (Functional Multiprocessing for NFS) 380.2 713.33 P
6.08 (Network Servers. Technical Report) 380.2 701.33 P
15.89 (Technical Report 1, Auspex) 380.2 689.33 P
(Engineering, July 1990.) 380.2 677.33 T
([Patterson88]) 315 659.33 T
1.23 (David) 380.2 659.33 P
1.23 (A. Patterson, Garth Gibson, and) 407.12 659.33 P
1.21 (Randy) 380.2 647.33 P
1.21 (H. Katz. A Case for Redundant) 408.79 647.33 P
-0.24 (Arrays of Inexpensive Disks \050RAID\051. In) 380.2 635.33 P
1 F
22.99 (International Conference on) 380.2 623.33 P
-0.09 (Management of Data \050SIGMOD\051) 380.2 611.33 P
0 F
-0.09 (, pages) 512.33 611.33 P
(109\320116, June 1988.) 380.2 599.33 T
([Sandberg85]) 315 581.33 T
6.91 (Russel Sandberg, David Goldberg,) 380.2 581.33 P
2.48 (Steve Kleiman, Dan Walsh, and Bob) 380.2 569.33 P
-0.57 (Lyon. Design and Implementation of the) 380.2 557.33 P
3.1 (Sun Network Filesystem. In) 380.2 545.33 P
1 F
3.1 (Summer) 507.25 545.33 P
(1985 Usenix Conference) 380.2 533.33 T
0 F
(, 1985.) 479.01 533.33 T
([Tweten90]) 315 515.33 T
3.38 (David Tweten. Hiding Mass Storage) 380.2 515.33 P
13.39 (Under Unix: NASA\325s MSS-II) 380.2 503.33 P
3.31 (Architecture.) 380.2 491.33 P
1 F
3.31 (Tenth IEEE Symposium) 438.45 491.33 P
0.81 (on Mass Storage) 380.2 479.33 P
0 F
0.81 (, pages 140\320145, May) 449.01 479.33 P
(1990.) 380.2 467.33 T
FMENDPAGE
%%EndPage: "11" 10
%%Page: "10" 10
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
0 10 Q
1.44 (Because of the 1.1 ms overhead and the high transfer) 72 713.33 P
0.44 (rate of HIPPI, network packet sizes of 120 KB are nec-) 72 703.33 P
-0.12 (essary to achieve 75% utilization of the HIPPI. In such a) 72 693.33 P
0.02 (situation, small packets \050less than 40 KB\051 are best trans-) 72 683.33 P
1.93 (ferred over a lower latency network for two reasons.) 72 673.33 P
2.88 (First, small packets waste a disproportionately lar) 72 663.33 P
2.88 (ge) 287.57 663.33 P
-0.18 (amount of the available HIPPI bandwidth. Second, since) 72 653.33 P
1.86 (lar) 72 643.33 P
1.86 (ge average HIPPI packets are needed to ef) 82.36 643.33 P
1.86 (\336ciently) 264.24 643.33 P
0.08 (utilize the HIPPI, small packets which encounter a lar) 72 633.33 P
0.08 (ge) 287.57 633.33 P
-0.17 (packet currently being transferred must wait many milli-) 72 623.33 P
0.66 (seconds before they can use the network. W) 72 613.33 P
0.66 (e thus con-) 251.82 613.33 P
2.16 (clude that having two networks bene\336ts performance) 72 603.33 P
0.5 (both for small and lar) 72 593.33 P
0.5 (ge packets, small packets because) 159.85 593.33 P
0.41 (they need not wait while lar) 72 583.33 P
0.41 (ge packets use the network,) 185.17 583.33 P
2.15 (lar) 72 573.33 P
2.15 (ge packets because the available bandwidth of the) 82.36 573.33 P
2.07 (HIPPI is not lost due to inef) 72 563.33 P
2.07 (\336cient small packets. In) 195.85 563.33 P
2.57 (addition, adding an Ethernet connection costs essen-) 72 553.33 P
0.5 (tially nothing, since all workstations already come with) 72 543.33 P
(an Ethernet connection.) 72 533.33 T
2.01 (Third, we examine how much the parity engine) 93.6 523.33 P
1.78 (enhances disk array write performance. T) 72 513.33 P
1.78 (o do so, we) 246.14 513.33 P
0.11 (compare read and write performance of RAID-II, which) 72 503.33 P
-0.12 (has a parity engine, to the read and write performance of) 72 493.33 P
0.5 (our \336rst prototype, RAID-I, which has no parity engine) 72 483.33 P
2.51 (and must use its Sun4 \336le server to compute parity) 72 473.33 P
0.34 ([Chervenak91]. Both RAID-II and RAID-I tests use the) 72 463.33 P
3.12 (same number of memory transfers.) 72 453.33 P
0 8 Q
2.5 (7) 224.1 457.33 P
0 10 Q
3.12 ( T) 228.1 453.33 P
3.12 (able 2 shows) 239.12 453.33 P
0.38 (RAID-II full-stripe writes [Lee91] are only 13% slower) 72 443.33 P
2.41 (than reads, where RAID-I full-stripe writes are 50%) 72 433.33 P
0.04 (slower than reads. W) 72 423.33 P
0.04 (ithout the parity engine, computing) 156.09 423.33 P
-0.07 (parity on RAID-II would have been even slower than on) 72 413.33 P
-0.08 (RAID-I, because the data over which parity is computed) 72 403.33 P
1.51 (resides on the XBUS card and would have to \336rst be) 72 393.33 P
1.68 (transferred over a VME link into the Sun4\325) 72 383.33 P
1.68 (s memory) 255.62 383.33 P
-0.15 (before parity could be computed. Even once data resides) 72 373.33 P
0.61 (in Sun4 memory) 72 363.33 P
0.61 (, parity can only be computed at a few) 139.2 363.33 P
-0.05 (megabytes per second. Clearly hardware parity is neces-) 72 353.33 P
(sary for high-performance in RAID Level 5 systems.) 72 343.33 T
0.34 (Last, we examine our decision to attach the HIPPI) 93.6 333.33 P
1.35 (network to the XBUS card instead of to the Sun4 \336le) 72 323.33 P
2.12 (server) 72 313.33 P
2.12 (. T) 95.87 313.33 P
2.12 (o do so, we again compare RAID-II perfor-) 108.39 313.33 P
3.22 (mance to RAID-I performance. T) 72 303.33 P
3.22 (able 2 shows that) 218.22 303.33 P
0.72 (RAID-II achieves 8.7 to 15.2 times the performance of) 72 293.33 P
1.25 (RAID-I. If we had simply connected the HIPPI to the) 72 283.33 P
0.94 (Sun4\325) 72 273.33 P
0.94 (s VME bus instead of to the XBUS card, RAID-) 95.33 273.33 P
-0.17 (II\325) 72 263.33 P
-0.17 (s performance would have been comparable to RAID-) 81.43 263.33 P
0.63 (I\325) 72 253.33 P
0.63 (s performance, because RAID-I\325) 78.11 253.33 P
0.63 (s performance is lim-) 209.89 253.33 P
-0.14 (ited by the speed of its memory bus, so using more disks) 72 243.33 P
2.11 (and a fast network would not have increased perfor-) 72 233.33 P
(mance [Chervenak91].) 72 223.33 T
2 12 Q
(4 Conclusions) 72 206 T
0 10 Q
1.04 (In this paper) 93.6 195.33 P
1.04 (, we have evaluated the performance) 145.25 195.33 P
0.18 (of RAID-II, a network-attached storage server) 72 185.33 P
0.18 (. W) 257.2 185.33 P
0.18 (e have) 271.01 185.33 P
3.76 (demonstrated end-to-end system performance of 20) 72 175.33 P
0.07 (MB/s for both disk array reads and writes. W) 72 165.33 P
0.07 (e have also) 252.46 165.33 P
2.72 (examined four major design issues in the design of) 72 155.33 P
2.52 (RAID-II. W) 72 145.33 P
2.52 (e \336rst measured contention for the main) 122.56 145.33 P
-0.23 (interconnect of RAID-II, the XBUS, and found mar) 72 135.33 P
-0.23 (ginal) 277.01 135.33 P
0.1 (levels of contention, due to the \336ne-grained interleaving) 72 125.33 P
72 102 297 122 C
72 110 204 110 2 L
0.25 H
2 Z
0 X
0 K
N
0 0 612 792 C
0 10 Q
0 X
0 K
-0.4 (7.  W) 72 95.33 P
-0.4 (e compensate for the lack of a fast network \050HIPPI\051) 92.32 95.33 P
(on RAID-I by replacing the network send with a mem-) 72 85.33 T
(ory to memory copy \050kernel to user space\051.) 72 75.33 T
1.64 (of the memory ports and the relatively lar) 315 713.33 P
1.64 (ge accesses) 492.57 713.33 P
0.48 (made by the HIPPI and parity ports. W) 315 703.33 P
0.48 (e next examined) 473.54 703.33 P
0.33 (the necessity of using two networks, one for high band-) 315 693.33 P
0.83 (width and one for low latency) 315 683.33 P
0.83 (. W) 438.12 683.33 P
0.83 (e concluded that, due) 452.58 683.33 P
1.19 (to the high bandwidth of the HIPPI network, even the) 315 673.33 P
1.45 (moderate 1.1 ms network overhead per request would) 315 663.33 P
1.29 (waste a lar) 315 653.33 P
1.29 (ge fraction of network bandwidth for small) 360.14 653.33 P
3.35 (requests, thus justifying our use of a second, low-) 315 643.33 P
2.34 (latency network. W) 315 633.33 P
2.34 (e then examined the performance) 397.43 633.33 P
1.06 (bene\336t of the parity computation engine for disk array) 315 623.33 P
0.16 (writes and concluded that it allowed full-stripe writes to) 315 613.33 P
3.64 (achieve almost the same performance as full-stripe) 315 603.33 P
1.27 (reads. Last, we examined the wisdom of attaching the) 315 593.33 P
0.64 (HIPPI directly to the XBUS card by comparing perfor-) 315 583.33 P
0.78 (mance to RAID-I. W) 315 573.33 P
0.78 (e concluded that this allowed sys-) 400.91 573.33 P
1.83 (tem performance to scale with the XBUS rather than) 315 563.33 P
(being limited by the speed of the Sun4 memory bus.) 315 553.33 T
1.69 (W) 336.6 543.33 P
1.69 (e have been quite pleased by the functionality) 345.23 543.33 P
0.82 (and performance of RAID-II. Using identical Sun4 \336le) 315 533.33 P
0.16 (servers and essentially the same disks and controllers as) 315 523.33 P
-0.04 (RAID-I, we achieved a speedup of 8 to 15 over RAID-I.) 315 513.33 P
-0.01 (The use of the XBUS card, with its custom XBUS inter-) 315 503.33 P
0.03 (connect and parity engine, enabled performance to scale) 315 493.33 P
1.62 (well beyond the limits of the Sun4 memory bus. The) 315 483.33 P
0.21 (XBUS card fully supported the bandwidth requirements) 315 473.33 P
0.51 (of 24 disks and achieved end-to-end performance of 20) 315 463.33 P
(MB/s.) 315 453.33 T
2 12 Q
(5 Acknowledgments) 315 436 T
0 10 Q
3.72 (This work was supported in part by NASA/-) 336.6 425.33 P
0.35 (DARP) 315 415.33 P
0.35 (A grant number NAG 2-591, NSF MIP 8715235,) 340.74 415.33 P
5.2 (and NFS Infrastructure Grant No. CDA-8722788.) 315 405.33 P
0.02 (RAID-II was made practical through donations by IBM,) 315 395.33 P
1.26 (Thinking Machines Corporation, IDT) 315 385.33 P
1.26 (, and Sun Micro-) 468.49 385.33 P
2.63 (systems. W) 315 375.33 P
2.63 (e gratefully acknowledge contributions in) 362.92 375.33 P
0.35 (the design of RAID-II by Garth Gibson, Rob P\336le, Rob) 315 365.33 P
0.54 (Quiros, and Mani V) 315 355.33 P
0.54 (aradarajan. Lance Lee at Interphase) 395.47 355.33 P
1.33 (and Steve Goodison at Andataco were instrumental in) 315 345.33 P
3.63 (providing quick delivery of \336ve Interphase Cougar) 315 335.33 P
(VME disk controllers for evaluation.) 315 325.33 T
2 12 Q
(6 Refer) 315 308 T
(ences) 352.41 308 T
0 10 Q
([Chervenak91]) 315 291.33 T
1.02 (Ann) 380.2 291.33 P
1.02 (L. Chervenak and Randy) 399.91 291.33 P
1.02 (H. Katz.) 505.4 291.33 P
-0.01 (Performance of a Disk Array Prototype.) 380.2 279.33 P
5.59 (In) 380.2 267.33 P
1 F
5.59 (Proceedings of the 1991 ACM) 396.61 267.33 P
21.05 (SIGMETRICS Conference on) 380.2 255.33 P
12.28 (Measurement and Modeling of) 380.2 243.33 P
3.48 (Computer Systems) 380.2 231.33 P
0 F
3.48 (, volume) 457.8 231.33 P
3.48 (19, pages) 498.77 231.33 P
7.65 (188\320197, May 1991. Performance) 380.2 219.33 P
(Evaluation Review.) 380.2 207.33 T
([Coleman90]) 315 189.33 T
1.64 (Sam Coleman and Steve Miller. Mass) 380.2 189.33 P
7.65 (Storage System Reference Model:) 380.2 177.33 P
(Version 4, May 1990.) 380.2 165.33 T
([Collins91]) 315 147.33 T
4.97 (Bill Collins, Lynn Jones, Granville) 380.2 147.33 P
-0.38 (Chorn, Ronald Christman, Danny Cook,) 380.2 135.33 P
4.63 (and Christina Mercier. Los Alamos) 380.2 123.33 P
1.73 (High-Performance Data System: Early) 380.2 111.33 P
1.09 (Experiences. Technical Report La-UR-) 380.2 99.33 P
(91-3590, Los Alamos, November 1991.) 380.2 87.33 T
FMENDPAGE
%%EndPage: "10" 9
%%Page: "9" 9
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
0 10 Q
2.62 (the unit of interleaving, the initial waiting period is) 72 713.33 P
(insigni\336cant relative to the total transfer time.) 72 703.33 T
0.6 (Performance suf) 93.6 693.33 P
0.6 (fers, however) 159.79 693.33 P
0.6 (, when the disk sub-) 214.38 693.33 P
0.9 (system begins to read data from the disks to the mem-) 72 683.33 P
1.35 (ory) 72 673.33 P
1.35 (. There are two factors leading to this degradation.) 84.67 673.33 P
2.39 (First, each VME port can sustain less than 8 MB/s,) 72 663.33 P
0.66 (which is less than one-fourth the bandwidth of a single) 72 653.33 P
0.26 (memory port. Memory requests from other XBUS ports) 72 643.33 P
0.89 (must hence wait behind the slow VME access Second,) 72 633.33 P
0.51 (we have con\336gured the disk controllers to transfer only) 72 623.33 P
2.21 (32 32-bit words per VME access. These two factors) 72 613.33 P
0.29 (cause the access pattern generated by a disk port is thus) 72 603.33 P
0.69 (similar to small, random memory accesses, resulting in) 72 593.33 P
0.83 (a much higher degree of contention for memory) 72 583.33 P
0.83 (. Since) 268.96 583.33 P
0.29 (the VME bus supports transfer sizes of up to 128 32-bit) 72 573.33 P
5.81 (words, XBUS and memory contention could be) 72 563.33 P
0.27 (decreased by increasing the size of VME accesses. This) 72 553.33 P
-0.08 (may decrease overall performance, however) 72 543.33 P
-0.08 (, by increas-) 248.04 543.33 P
1.18 (ing the time control words wait for the VME bus. W) 72 533.33 P
1.18 (e) 292.56 533.33 P
72 322.2 540 529.88 C
75.6 435.38 535.5 544.68 R
7 X
0 K
V
2 10 Q
0 X
-0.42 (Disk Array Read) 215.84 508.02 P
(Performance) 224.46 496.02 T
(Disk Array W) 304.71 508.02 T
(rite) 365.05 508.02 T
(Performance) 314.62 496.02 T
-0.23 (W) 396.98 508.02 P
-0.23 (rite Performance) 406.79 508.02 P
(Degradation) 411.72 496.02 T
0 F
(RAID-I) 150.4 480.02 T
(2.4 MB/s) 235.68 480.02 T
(1.2 MB/s) 325.84 480.02 T
(50%) 429.21 480.02 T
(RAID-II) 148.74 464.02 T
(20.9 MB/s) 231.1 464.02 T
(18.2 MB/s) 321.27 464.02 T
(13%) 429.21 464.02 T
(RAID-II speedup) 131.11 448.02 T
(8.7) 245.96 448.02 T
(15.2) 333.62 448.02 T
123.28 522.43 123.28 442.93 2 L
V
0.5 H
0 Z
N
207.35 522.43 207.35 442.93 2 L
V
N
209.85 522.43 209.85 442.93 2 L
V
N
295.81 522.93 295.81 442.43 2 L
V
N
387.68 522.43 387.68 442.93 2 L
V
N
390.18 522.43 390.18 442.93 2 L
V
N
487.82 522.43 487.82 457.18 2 L
V
N
123.03 522.68 488.07 522.68 2 L
V
N
123.53 491.93 487.57 491.93 2 L
V
N
123.53 489.43 487.57 489.43 2 L
V
N
123.03 474.68 488.07 474.68 2 L
V
N
123.53 459.93 487.57 459.93 2 L
V
N
123.53 457.43 487.57 457.43 2 L
V
N
123.03 442.68 390.43 442.68 2 L
V
N
75.6 327.38 535.5 432.48 R
7 X
V
2 11 Q
0 X
0.08 (T) 89.35 425.82 P
0.08 (able 1) 95.67 425.82 P
2 10 Q
0.08 (: Performance Comparison between RAID-II and RAID-I) 123.53 425.82 P
0 F
0.08 (. This table compares the performance of) 371.02 425.82 P
0.09 (RAID-II to that of RAID-I. Because RAID-II has a special purpose parity engine, disk array write performance) 89.35 415.82 P
0.2 (is comparable to disk array read performance. All writes in this test are full-stripe writes [Lee91]. For RAID-II) 89.35 405.82 P
0.84 (reads, data is read from the disk array into XBUS memory then sent over the HIPPI network back to XBUS) 89.35 395.82 P
0.85 (memory) 89.35 385.82 P
0.85 (. For RAID-I reads, data is read from the disk array into Sun4 memory) 122.01 385.82 P
0.85 (, then copied again into Sun4) 414.67 385.82 P
0.81 (memory) 89.35 375.82 P
0.81 (. This extra copy equalized the number of memory accesses per data word. For RAID-II writes, data) 122.01 375.82 P
1.06 (starts in XBUS memory) 89.35 365.82 P
1.06 (, is sent over HIPPI back into XBUS memory) 188.22 365.82 P
1.06 (, parity is computed, and the data and) 378.45 365.82 P
0.16 (parity are written to the disk subsystem. For RAID-I writes, data starts in Sun4 memory) 89.35 355.82 P
0.16 (, gets copied to another) 441.86 355.82 P
-0.05 (location in Sun4 memory) 89.35 345.82 P
-0.05 (, then is written to disk. Meanwhile, parity is computed on the Sun4. RAID-I uses a 32) 189.88 345.82 P
(KB striping unit with 8 disks; RAID-II uses a 64 KB striping unit with 24 disks.) 89.35 335.82 T
0 0 612 792 C
72 72 540 322.2 C
72 101.7 535.5 318.6 R
7 X
0 K
V
2 10 Q
0 X
(T) 183.13 269.93 T
(est) 188.88 269.93 T
(Aggr) 290.62 281.93 T
(egate XBUS) 312.09 281.93 T
(Bandwidth) 303.42 269.93 T
(A) 396.7 281.93 T
(verage Bandwidth) 403.18 281.93 T
(Per Active Subsystem) 392.59 269.93 T
0 F
(HIPPID + HIPPIS + Parity Engine) 122.48 253.93 T
(1) 307.36 253.93 T
(14 MB/s) 311.99 253.93 T
(38 MB/s) 421.59 253.93 T
-0.15 (HIPPID + HIPPIS + Parity Engine +) 118.41 237.93 P
(Disk Subsystem) 159.48 225.93 T
(1) 307.36 237.93 T
(10 MB/s) 311.99 237.93 T
(28 MB/s) 421.59 237.93 T
110.98 296.35 110.98 220.85 2 L
V
0.5 H
0 Z
N
271.43 296.35 271.43 220.85 2 L
V
N
273.93 296.35 273.93 220.85 2 L
V
N
381.38 296.85 381.38 220.35 2 L
V
N
496.52 296.35 496.52 220.85 2 L
V
N
110.73 296.6 496.77 296.6 2 L
V
N
111.23 265.85 496.27 265.85 2 L
V
N
111.23 263.35 496.27 263.35 2 L
V
N
110.73 248.6 496.77 248.6 2 L
V
N
110.73 220.6 496.77 220.6 2 L
V
N
72 106.2 535.5 210.6 R
7 X
V
2 11 Q
0 X
-0.06 (T) 85.75 203.93 P
-0.06 (able 2: XBUS contention) 92.07 203.93 P
0 10 Q
-0.06 (. When using only the HIPPID, HIPPIS, and parity engine, the XBUS experiences) 207.54 203.93 P
0.3 (no contention for the memory banks, supporting the full bandwidth of each port. However) 85.75 193.93 P
0.3 (, performance suf) 449.88 193.93 P
0.3 (fers) 520.52 193.93 P
0.96 (when the disk subsystem is simultaneously reading data from the disks to the memory) 85.75 183.93 P
0.96 (. There are two factors) 441.49 183.93 P
0.12 (leading to this degradation. First, each VME port can sustain less than 8 MB/s, which is less than one-fourth the) 85.75 173.93 P
0.26 (bandwidth of a single memory port. Memory requests from other XBUS ports must hence wait behind the slow) 85.75 163.93 P
0.31 (VME access Second, we have con\336gured the disk controllers to transfer only 32 32-bit words per VME access.) 85.75 153.93 P
1.01 (These two factors cause the access pattern generated by a disk port is thus similar to small, random memory) 85.75 143.93 P
-0.19 (accesses, resulting in a much higher degree of contention for memory) 85.75 133.93 P
-0.19 (. Even with this degradation, the XBUS can) 361.64 133.93 P
1.2 (still support 37 MB/s of disk write \050using only the HIPPID, parity engine, and disk subsystem\051 or disk read) 85.75 123.93 P
(activity \050using the HIPPIS and disk subsystem\051.) 85.75 113.93 T
0 0 612 792 C
0 10 Q
0 X
0 K
0.51 (are currently tuning performance by varying the size of) 315 713.33 P
(VME accesses.) 315 703.33 T
2.39 (T) 336.6 693.33 P
2.39 (able 1 shows XBUS contention when all four) 342.01 693.33 P
2.24 (ports \050HIPPID, HIPPIS, parity engine, and disk sub-) 315 683.33 P
1.27 (system\051 are active. Real application environments will) 315 673.33 P
1.07 (not have the network con\336gured to loop back, so only) 315 663.33 P
0.46 (one of the network ports would be active for disk reads) 315 653.33 P
0.05 (or disk writes. In such an) 315 643.33 P
0.05 (environment, the XBUS could) 418.25 643.33 P
2.1 (easily support the full port bandwidth \05038 MB/s\051 for) 315 633.33 P
1.75 (both disk reads \050exercising the HIPPIS and disk sub-) 315 623.33 P
1.77 (system\051 or disk writes \050exercising the HIPPID, parity) 315 613.33 P
(engine, and disk subsystem\051.) 315 603.33 T
2 (Next, we examine the bene\336t of using two net-) 336.6 593.33 P
0.37 (works, one for high bandwidth and one for low latency) 315 583.33 P
0.37 (.) 537.5 583.33 P
1.98 (W) 315 573.33 P
1.98 (e saw in Figure 3 that the overhead of initiating a) 323.64 573.33 P
-0.19 (HIPPI access was a surprisingly low 1.1 ms, comparable) 315 563.33 P
2.21 (tothe Ethernet latency of about 0.5 ms. In this short) 315 553.33 P
0.04 (time, however) 315 543.33 P
0.04 (, the HIPPI could have transferred 40 KB.) 371.82 543.33 P
FMENDPAGE
%%EndPage: "9" 8
%%Page: "8" 8
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
0 10 Q
1.13 (riences no contention for memory and can support the) 72 713.33 P
0.63 (full bandwidth of each \05038 MB/s\051. This lack of conten-) 72 703.33 P
0.6 (tion is due to two reasons. First, memory is interleaved) 72 693.33 P
0.2 (on a \336ne-grained basis \05016 words\051, so the memory ports) 72 683.33 P
1.99 (are evenly load balanced; that is, no memory port is) 72 673.33 P
0.82 (more heavily loaded than any other memory port. Sec-) 72 663.33 P
72 72 540 657 C
75.6 76.5 535.5 156.8 R
7 X
0 K
V
2 11 Q
0 X
1.38 (Figur) 75.6 150.13 P
1.38 (e 6: Disk W) 101.65 150.13 P
1.38 (rite Performance) 160.2 150.13 P
0 10 Q
1.25 (. This \336gure shows how disk write performance scales with increasing) 241.8 150.13 P
0.23 (numbers of disks. Figure \050a\051 uses one string on one Interphase Cougar and varies the number of disks from 1 to 5.) 75.6 140.13 P
0.73 (Figure \050b\051 \336xes the number of disks per string at three and varies the number of strings per Cougar from 1 to 2.) 75.6 130.13 P
-0.16 (Figure \050c\051 varies the number of Cougars per VME bus from 1 to 2, \336xing the number of disks per string at three and) 75.6 120.13 P
0.36 (the number of strings per Cougar at two. Figure \050d\051 varies the number of VME busses attached to the XBUS card) 75.6 110.13 P
0.6 (from one to four) 75.6 100.13 P
0.6 (, \336xing the number of disks per string at three, the number of strings per Cougar at two, and the) 142.77 100.13 P
1.68 (number of Cougars per VME bus at one. These tests measure raw disk write performance\321the disk array is) 75.6 90.13 P
(con\336gured without redundancy) 75.6 80.13 T
(. Dotted lines show linear speedup.) 199.86 80.13 T
236.54 550.95 236.54 553.35 2 L
7 X
V
1 H
2 Z
0 X
N
235.34 552.15 237.74 552.15 2 L
7 X
V
0 X
N
209.98 551.17 209.98 553.57 2 L
7 X
V
0 X
N
208.78 552.37 211.18 552.37 2 L
7 X
V
0 X
N
183.42 548.68 183.42 551.08 2 L
7 X
V
0 X
N
182.22 549.88 184.62 549.88 2 L
7 X
V
0 X
N
156.86 532.53 156.86 534.93 2 L
7 X
V
0 X
N
155.66 533.73 158.06 533.73 2 L
7 X
V
0 X
N
130.3 497.62 130.3 500.02 2 L
7 X
V
0 X
N
129.1 498.82 131.5 498.82 2 L
7 X
V
0 X
N
236.54 552.15 209.98 552.37 183.42 549.88 156.86 533.73 130.3 498.82 5 L
7 X
V
0 X
N
103.74 465.7 103.74 468.1 2 L
7 X
V
0 X
N
102.54 466.9 104.94 466.9 2 L
7 X
V
0 X
N
236.54 625.28 236.54 627.68 2 L
7 X
V
0 X
N
235.34 626.48 237.74 626.48 2 L
7 X
V
0 X
N
103.74 466.9 236.54 626.48 2 L
7 X
V
0.5 H
9 X
N
103.74 466.9 236.54 466.9 2 L
7 X
V
2 H
0 X
N
103.74 466.9 103.74 629.69 2 L
7 X
V
0 X
N
103.74 466.9 103.74 465.3 2 L
7 X
V
0 X
N
0 12 Q
(0) 100.74 454.23 T
130.3 466.9 130.3 465.3 2 L
7 X
V
0 X
N
(1) 127.3 454.23 T
156.86 466.9 156.86 465.3 2 L
7 X
V
0 X
N
(2) 153.86 454.23 T
183.42 466.9 183.42 465.3 2 L
7 X
V
0 X
N
(3) 180.42 454.23 T
209.98 466.9 209.98 465.3 2 L
7 X
V
0 X
N
(4) 206.98 454.23 T
236.54 466.9 236.54 465.3 2 L
7 X
V
0 X
N
(5) 233.54 454.23 T
103.74 466.9 102.14 466.9 2 L
7 X
V
0 X
N
(0) 91.74 463.63 T
103.74 494.03 102.14 494.03 2 L
7 X
V
0 X
N
(1) 91.74 490.76 T
103.74 521.17 102.14 521.17 2 L
7 X
V
0 X
N
(2) 91.74 517.89 T
103.74 548.3 102.14 548.3 2 L
7 X
V
0 X
N
(3) 91.74 545.02 T
103.74 575.43 102.14 575.43 2 L
7 X
V
0 X
N
(4) 91.74 572.16 T
103.74 602.56 102.14 602.56 2 L
7 X
V
0 X
N
(5) 91.74 599.29 T
103.74 629.69 102.14 629.69 2 L
7 X
V
0 X
N
(6) 91.74 626.42 T
0 14 Q
(Number of Disks \050One String\051) 85.03 442.35 T
(Throughput \050MB/s\051) 0 -270 88.74 493.3 TF
479.4 599.94 479.4 602.34 2 L
7 X
V
1 H
0 X
N
478.2 601.14 480.61 601.14 2 L
7 X
V
0 X
N
413 540.43 413 542.83 2 L
7 X
V
0 X
N
411.8 541.63 414.2 541.63 2 L
7 X
V
0 X
N
479.4 601.14 413 541.63 2 L
7 X
V
0 X
N
346.61 469.3 346.61 471.7 2 L
7 X
V
0 X
N
345.41 470.5 347.81 470.5 2 L
7 X
V
0 X
N
479.4 611.55 479.4 613.95 2 L
7 X
V
0 X
N
478.2 612.75 480.61 612.75 2 L
7 X
V
0 X
N
346.61 470.5 479.4 612.75 2 L
7 X
V
0.5 H
9 X
N
346.61 470.5 479.4 470.5 2 L
7 X
V
2 H
0 X
N
346.61 470.5 346.61 633.29 2 L
7 X
V
0 X
N
346.61 470.5 346.61 468.9 2 L
7 X
V
0 X
N
0 12 Q
(0) 343.61 455.83 T
413 470.5 413 468.9 2 L
7 X
V
0 X
N
(1) 410.01 455.83 T
479.4 470.5 479.4 468.9 2 L
7 X
V
0 X
N
(2) 476.4 455.83 T
346.61 470.5 345.01 470.5 2 L
7 X
V
0 X
N
(0) 331.61 467.23 T
346.61 493.76 345.01 493.76 2 L
7 X
V
0 X
N
(1) 331.61 490.48 T
346.61 517.01 345.01 517.01 2 L
7 X
V
0 X
N
(2) 331.61 513.74 T
346.61 540.27 345.01 540.27 2 L
7 X
V
0 X
N
(3) 331.61 537 T
346.61 563.53 345.01 563.53 2 L
7 X
V
0 X
N
(4) 331.61 560.25 T
346.61 586.78 345.01 586.78 2 L
7 X
V
0 X
N
(5) 331.61 583.51 T
346.61 610.04 345.01 610.04 2 L
7 X
V
0 X
N
(6) 331.61 606.76 T
346.61 633.29 345.01 633.29 2 L
7 X
V
0 X
N
(7) 331.61 630.02 T
0 14 Q
(Number of Strings) 360.92 443.95 T
(Throughput \050MB/s\051) 0 -270 326.51 501.51 TF
232.93 268.96 232.93 271.35 2 L
7 X
V
1 H
0 X
N
231.74 270.16 234.13 270.16 2 L
7 X
V
0 X
N
169.34 266.62 169.34 269.03 2 L
7 X
V
0 X
N
168.14 267.83 170.54 267.83 2 L
7 X
V
0 X
N
232.93 270.16 169.34 267.83 2 L
7 X
V
0 X
N
105.74 220.9 105.74 223.3 2 L
7 X
V
0 X
N
104.54 222.1 106.94 222.1 2 L
7 X
V
0 X
N
232.93 312.35 232.93 314.75 2 L
7 X
V
0 X
N
231.74 313.55 234.13 313.55 2 L
7 X
V
0 X
N
105.74 222.1 232.93 313.55 2 L
7 X
V
0.5 H
9 X
N
105.74 222.1 232.93 222.1 2 L
7 X
V
2 H
0 X
N
105.74 222.1 105.74 384.89 2 L
7 X
V
0 X
N
105.74 222.1 105.74 220.5 2 L
7 X
V
0 X
N
0 12 Q
(0) 102.74 209.43 T
169.34 222.1 169.34 220.5 2 L
7 X
V
0 X
N
(1) 166.34 209.43 T
232.93 222.1 232.93 220.5 2 L
7 X
V
0 X
N
(2) 229.94 209.43 T
105.74 222.1 104.14 222.1 2 L
7 X
V
0 X
N
(0) 93.74 218.83 T
105.74 262.8 104.14 262.8 2 L
7 X
V
0 X
N
(5) 93.74 259.53 T
105.74 303.5 104.14 303.5 2 L
7 X
V
0 X
N
(10) 90.75 300.23 T
105.74 344.2 104.14 344.2 2 L
7 X
V
0 X
N
(15) 90.75 340.92 T
105.74 384.89 104.14 384.89 2 L
7 X
V
0 X
N
(20) 90.75 381.62 T
0 14 Q
(Number of Controllers) 105.6 195.55 T
(Throughput \050MB/s\051) 0 -270 88.14 248.5 TF
460.48 345.17 460.48 347.57 2 L
7 X
V
1 H
0 X
N
459.28 346.37 461.68 346.37 2 L
7 X
V
0 X
N
447.76 336.15 447.76 338.55 2 L
7 X
V
0 X
N
446.56 337.35 448.96 337.35 2 L
7 X
V
0 X
N
422.33 310.32 422.33 312.72 2 L
7 X
V
0 X
N
421.12 311.52 423.52 311.52 2 L
7 X
V
0 X
N
396.89 282.3 396.89 284.7 2 L
7 X
V
0 X
N
395.69 283.49 398.09 283.49 2 L
7 X
V
0 X
N
371.45 251.34 371.45 253.74 2 L
7 X
V
0 X
N
370.25 252.54 372.65 252.54 2 L
7 X
V
0 X
N
460.48 346.37 447.76 337.35 422.33 311.52 396.89 283.49 371.45 252.54 5 L
7 X
V
0 X
N
346.01 220.7 346.01 223.1 2 L
7 X
V
0 X
N
344.81 221.9 347.21 221.9 2 L
7 X
V
0 X
N
460.48 358.58 460.48 360.98 2 L
7 X
V
0 X
N
459.28 359.77 461.68 359.77 2 L
7 X
V
0 X
N
346.01 221.9 460.48 359.77 2 L
7 X
V
0.5 H
9 X
N
346.01 221.9 473.2 221.9 2 L
7 X
V
2 H
0 X
N
346.01 221.9 346.01 384.69 2 L
7 X
V
0 X
N
346.01 221.9 346.01 220.3 2 L
7 X
V
0 X
N
0 12 Q
(0) 343.01 208.23 T
388.41 221.9 388.41 220.3 2 L
7 X
V
0 X
N
(10) 382.41 208.23 T
430.8 221.9 430.8 220.3 2 L
7 X
V
0 X
N
(20) 424.81 208.23 T
473.2 221.9 473.2 220.3 2 L
7 X
V
0 X
N
(30) 467.2 208.23 T
346.01 221.9 344.41 221.9 2 L
7 X
V
0 X
N
(0) 331.01 218.63 T
346.01 276.17 344.41 276.17 2 L
7 X
V
0 X
N
(10) 328.01 272.89 T
346.01 330.43 344.41 330.43 2 L
7 X
V
0 X
N
(20) 328.01 327.16 T
346.01 384.69 344.41 384.69 2 L
7 X
V
0 X
N
(30) 328.01 381.42 T
0 14 Q
(Number of Disks) 361.42 195.35 T
(Throughput \050MB/s\051) 0 -270 322.41 248.3 TF
0 12 Q
(\050a\051 V) 81.83 423.49 T
(arying Disks Per String) 105.47 423.49 T
(\050b\051 V) 322.68 423.49 T
(arying Strings Per Controller) 346.99 423.49 T
(\050c\051 V) 81.83 169.73 T
(arying Controllers Per VME Bus) 105.47 169.73 T
(\050d\051 V) 299.36 169.73 T
(arying Number of Disks in System) 323.67 169.73 T
0 0 612 792 C
0 10 Q
0 X
0 K
0.04 (ond, if two accesses collide by trying to access the same) 315 713.33 P
1.34 (memory port, one access) 315 703.33 P
1.34 (must wait for the other) 420.88 703.33 P
1.34 (. But,) 516.72 703.33 P
2.13 (after this initial waiting period, the loser follows the) 315 693.33 P
3.83 (winner around the memory banks deterministically) 315 683.33 P
3.83 (.) 537.5 683.33 P
1.5 (Because the average access is many times lar) 315 673.33 P
1.5 (ger than) 506.02 673.33 P
FMENDPAGE
%%EndPage: "8" 7
%%Page: "7" 7
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
3 12 Q
(\245) 72 713.33 T
0 10 Q
(Does memory/XBUS contention signi\336cantly) 85.75 713.33 T
(degrade system performance?) 85.75 701.33 T
3 12 Q
(\245) 72 683.33 T
0 10 Q
(Is it necessary to have two networks: one \050HIPPI\051) 85.75 683.33 T
(attached to the XBUS card for high bandwidth, one) 85.75 671.33 T
-0.3 (\050Ethernet or FDDI\051 attached to the \336le server for low) 85.75 659.33 P
(latency?) 85.75 647.33 T
72 72 540 626.4 C
75.6 72.9 535.5 144.6 R
7 X
0 K
V
2 11 Q
0 X
1.85 (Figur) 75.6 137.93 P
1.85 (e 5: Disk Read Performance) 101.65 137.93 P
0 10 Q
1.68 (. This \336gure shows how disk read performance scales with increasing) 240.84 137.93 P
0.23 (numbers of disks. Figure \050a\051 uses one string on one Interphase Cougar and varies the number of disks from 1 to 5.) 75.6 127.93 P
0.73 (Figure \050b\051 \336xes the number of disks per string at three and varies the number of strings per Cougar from 1 to 2.) 75.6 117.93 P
-0.16 (Figure \050c\051 varies the number of Cougars per VME bus from 1 to 2, \336xing the number of disks per string at three and) 75.6 107.93 P
0.36 (the number of strings per Cougar at two. Figure \050d\051 varies the number of VME busses attached to the XBUS card) 75.6 97.93 P
0.6 (from one to four) 75.6 87.93 P
0.6 (, \336xing the number of disks per string at three, the number of strings per Cougar at two, and the) 142.77 87.93 P
(number of Cougars per VME bus at one. Dotted lines show linear speedup.) 75.6 77.93 T
245.51 537.54 245.51 539.95 2 L
7 X
V
1 H
2 Z
0 X
N
244.31 538.74 246.71 538.74 2 L
7 X
V
0 X
N
218.95 534.02 218.95 536.42 2 L
7 X
V
0 X
N
217.75 535.22 220.15 535.22 2 L
7 X
V
0 X
N
192.39 527.05 192.39 529.45 2 L
7 X
V
0 X
N
191.19 528.25 193.59 528.25 2 L
7 X
V
0 X
N
165.83 518.16 165.83 520.56 2 L
7 X
V
0 X
N
164.63 519.36 167.03 519.36 2 L
7 X
V
0 X
N
139.27 487.84 139.27 490.24 2 L
7 X
V
0 X
N
138.07 489.04 140.48 489.04 2 L
7 X
V
0 X
N
245.51 538.74 218.95 535.22 192.39 528.25 165.83 519.36 139.27 489.04 5 L
7 X
V
0 X
N
112.72 455.93 112.72 458.33 2 L
7 X
V
0 X
N
111.52 457.13 113.92 457.13 2 L
7 X
V
0 X
N
245.51 615.5 245.51 617.9 2 L
7 X
V
0 X
N
244.31 616.7 246.71 616.7 2 L
7 X
V
0 X
N
112.72 457.13 245.51 616.7 2 L
7 X
V
0.5 H
9 X
N
112.72 457.13 245.51 457.13 2 L
7 X
V
2 H
0 X
N
112.72 457.13 112.72 619.91 2 L
7 X
V
0 X
N
112.72 457.13 112.72 455.53 2 L
7 X
V
0 X
N
0 12 Q
(0) 109.72 441.82 T
139.27 457.13 139.27 455.53 2 L
7 X
V
0 X
N
(1) 136.28 441.82 T
165.83 457.13 165.83 455.53 2 L
7 X
V
0 X
N
(2) 162.84 441.82 T
192.39 457.13 192.39 455.53 2 L
7 X
V
0 X
N
(3) 189.39 441.82 T
218.95 457.13 218.95 455.53 2 L
7 X
V
0 X
N
(4) 215.95 441.82 T
245.51 457.13 245.51 455.53 2 L
7 X
V
0 X
N
(5) 242.51 441.82 T
112.72 457.13 111.12 457.13 2 L
7 X
V
0 X
N
(0) 99.72 453.02 T
112.72 484.26 111.12 484.26 2 L
7 X
V
0 X
N
(1) 99.72 480.15 T
112.72 511.39 111.12 511.39 2 L
7 X
V
0 X
N
(2) 99.72 507.28 T
112.72 538.52 111.12 538.52 2 L
7 X
V
0 X
N
(3) 99.72 534.41 T
112.72 565.65 111.12 565.65 2 L
7 X
V
0 X
N
(4) 99.72 561.54 T
112.72 592.78 111.12 592.78 2 L
7 X
V
0 X
N
(5) 99.72 588.67 T
112.72 619.91 111.12 619.91 2 L
7 X
V
0 X
N
(6) 99.72 615.8 T
0 14 Q
(Number of Disks \050One String\051) 94 430.22 T
(Throughput \050MB/s\051) 0 -270 90.92 483.52 TF
(Number of Controllers) 112.05 197.46 T
0 12 Q
(\050a\051 V) 100.28 408.6 T
(arying Disks Per String) 123.91 408.6 T
(\050b\051 V) 323.14 408.6 T
(arying Strings Per Controller) 347.45 408.6 T
(\050c\051 V) 100.28 166.84 T
(arying Controllers Per VME Bus) 123.91 166.84 T
(\050d\051 V) 320.8 166.84 T
(arying Number of Disks in) 345.11 166.84 T
(System) 364.27 153 T
474.7 599.96 474.7 602.36 2 L
7 X
V
1 H
0 X
N
473.5 601.16 475.9 601.16 2 L
7 X
V
0 X
N
408.3 527.11 408.3 529.51 2 L
7 X
V
0 X
N
407.11 528.31 409.51 528.31 2 L
7 X
V
0 X
N
474.7 601.16 408.3 528.31 2 L
7 X
V
0 X
N
341.91 455.65 341.91 458.05 2 L
7 X
V
0 X
N
340.71 456.85 343.11 456.85 2 L
7 X
V
0 X
N
474.7 598.58 474.7 600.98 2 L
7 X
V
0 X
N
473.5 599.78 475.9 599.78 2 L
7 X
V
0 X
N
341.91 456.85 474.7 599.78 2 L
7 X
V
0.5 H
9 X
N
341.91 456.85 474.7 456.85 2 L
7 X
V
2 H
0 X
N
341.91 456.85 341.91 619.64 2 L
7 X
V
0 X
N
341.91 456.85 341.91 455.25 2 L
7 X
V
0 X
N
(0) 338.91 444.34 T
408.3 456.85 408.3 455.25 2 L
7 X
V
0 X
N
(1) 405.31 444.34 T
474.7 456.85 474.7 455.25 2 L
7 X
V
0 X
N
(2) 471.7 444.34 T
341.91 456.85 340.31 456.85 2 L
7 X
V
0 X
N
(0) 328.91 452.74 T
341.91 483.98 340.31 483.98 2 L
7 X
V
0 X
N
(1) 328.91 479.87 T
341.91 511.11 340.31 511.11 2 L
7 X
V
0 X
N
(2) 328.91 507 T
341.91 538.24 340.31 538.24 2 L
7 X
V
0 X
N
(3) 328.91 534.14 T
341.91 565.38 340.31 565.38 2 L
7 X
V
0 X
N
(4) 328.91 561.27 T
341.91 592.51 340.31 592.51 2 L
7 X
V
0 X
N
(5) 328.91 588.4 T
341.91 619.64 340.31 619.64 2 L
7 X
V
0 X
N
(6) 328.91 615.53 T
0 14 Q
(Number of Strings) 356.23 429.74 T
(Throughput \050MB/s\051) 0 -270 317.91 483.25 TF
240.01 282.77 240.01 285.17 2 L
7 X
V
1 H
0 X
N
238.81 283.97 241.21 283.97 2 L
7 X
V
0 X
N
176.41 269.52 176.41 271.92 2 L
7 X
V
0 X
N
175.21 270.72 177.61 270.72 2 L
7 X
V
0 X
N
240.01 283.97 176.41 270.72 2 L
7 X
V
0 X
N
112.81 226.23 112.81 228.63 2 L
7 X
V
0 X
N
111.61 227.43 114.01 227.43 2 L
7 X
V
0 X
N
240.01 312.81 240.01 315.21 2 L
7 X
V
0 X
N
238.81 314.01 241.21 314.01 2 L
7 X
V
0 X
N
112.81 227.43 240.01 314.01 2 L
7 X
V
0.5 H
9 X
N
112.81 227.43 240.01 227.43 2 L
7 X
V
2 H
0 X
N
112.81 227.43 112.81 390.22 2 L
7 X
V
0 X
N
112.81 227.43 112.81 225.83 2 L
7 X
V
0 X
N
0 12 Q
(0) 109.82 211.92 T
176.41 227.43 176.41 225.83 2 L
7 X
V
0 X
N
(1) 173.41 211.92 T
240.01 227.43 240.01 225.83 2 L
7 X
V
0 X
N
(2) 237.01 211.92 T
112.81 227.43 111.21 227.43 2 L
7 X
V
0 X
N
(0) 99.82 223.32 T
112.81 268.12 111.21 268.12 2 L
7 X
V
0 X
N
(5) 99.82 264.01 T
112.81 308.82 111.21 308.82 2 L
7 X
V
0 X
N
(10) 96.82 304.71 T
112.81 349.52 111.21 349.52 2 L
7 X
V
0 X
N
(15) 96.82 345.41 T
112.81 390.22 111.21 390.22 2 L
7 X
V
0 X
N
(20) 96.82 386.11 T
0 14 Q
(Throughput \050MB/s\051) 0 -270 95.21 253.83 TF
343.31 227.44 457.79 357.32 2 L
7 X
V
0.5 H
9 X
N
457.79 354.88 457.79 357.28 2 L
7 X
V
1 H
0 X
N
456.59 356.08 458.99 356.08 2 L
7 X
V
0 X
N
445.07 340.04 445.07 342.44 2 L
7 X
V
0 X
N
443.87 341.24 446.27 341.24 2 L
7 X
V
0 X
N
419.63 312.41 419.63 314.81 2 L
7 X
V
0 X
N
418.43 313.61 420.83 313.61 2 L
7 X
V
0 X
N
394.19 283.97 394.19 286.37 2 L
7 X
V
0 X
N
392.99 285.17 395.39 285.17 2 L
7 X
V
0 X
N
368.75 255.1 368.75 257.5 2 L
7 X
V
0 X
N
367.55 256.3 369.95 256.3 2 L
7 X
V
0 X
N
457.79 356.08 445.07 341.24 419.63 313.61 394.19 285.17 368.75 256.3 5 L
7 X
V
0 X
N
343.31 226.24 343.31 228.64 2 L
7 X
V
0 X
N
342.11 227.44 344.51 227.44 2 L
7 X
V
0 X
N
457.79 356.12 457.79 358.52 2 L
7 X
V
0 X
N
456.59 357.32 458.99 357.32 2 L
7 X
V
0 X
N
343.31 227.44 470.51 227.44 2 L
7 X
V
2 H
0 X
N
343.31 227.44 343.31 390.23 2 L
7 X
V
0 X
N
343.31 227.44 343.31 225.84 2 L
7 X
V
0 X
N
0 12 Q
(0) 340.32 215.93 T
385.71 227.44 385.71 225.84 2 L
7 X
V
0 X
N
(10) 379.71 215.93 T
428.11 227.44 428.11 225.84 2 L
7 X
V
0 X
N
(20) 422.11 215.93 T
470.51 227.44 470.51 225.84 2 L
7 X
V
0 X
N
(30) 464.51 215.93 T
343.31 227.44 341.71 227.44 2 L
7 X
V
0 X
N
(0) 331.32 223.33 T
343.31 281.71 341.71 281.71 2 L
7 X
V
0 X
N
(10) 328.32 277.6 T
343.31 335.97 341.71 335.97 2 L
7 X
V
0 X
N
(20) 328.32 331.86 T
343.31 390.23 341.71 390.23 2 L
7 X
V
0 X
N
(30) 328.32 386.12 T
0 14 Q
(Number of Disks) 358.72 200.33 T
(Throughput \050MB/s\051) 0 -270 325.71 253.84 TF
0 0 612 792 C
3 12 Q
0 X
0 K
(\245) 315 713.33 T
0 10 Q
(How much does the parity computation engine) 328.74 713.33 T
(improve performance?) 328.74 701.33 T
3 12 Q
(\245) 315 683.33 T
0 10 Q
(Does attaching the network to the XBUS card) 328.74 683.33 T
(instead of the \336le server increase performance?) 328.74 671.33 T
4.91 (First, we examine memory/XBUS contention.) 336.6 659.33 P
-0.15 (T) 315 649.33 P
-0.15 (able 1 shows that when the HIPPID, HIPPIS and parity) 320.41 649.33 P
0.1 (engine ports are simultaneously active, the XBUS expe-) 315 639.33 P
FMENDPAGE
%%EndPage: "7" 6
%%Page: "6" 6
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
70.99 395.6 298.01 712.4 C
74.39 402.8 294.19 504.99 R
7 X
0 K
V
2 11 Q
0 X
2.09 (Figur) 74.39 498.33 P
2.09 (e 3) 100.44 498.33 P
2 10 Q
1.9 (: HIPPI Loopback) 115.65 498.33 P
0 F
1.9 (This \336gure shows the) 202.71 498.33 P
0.32 (performance of the HIPPI network. Data is transferred) 74.39 488.33 P
0.46 (from the XBUS memory to the HIPPI source board to) 74.39 478.33 P
4.39 (the HIPPI destination board and back to XBUS) 74.39 468.33 P
1.36 (memory) 74.39 458.33 P
1.36 (. The overhead of sending a HIPPI packet is) 107.05 458.33 P
0.82 (about 1.1 ms, mostly due to setting up the HIPPI and) 74.39 448.33 P
1.1 (XBUS control registers. The measured data is shown) 74.39 438.33 P
2.4 (as symbols; the dashed line represents performance) 74.39 428.33 P
0.27 (derived from a simple model with a constant overhead) 74.39 418.33 P
(of 1.1 ms and a maximum throughput of 38.5 MB/s.) 74.39 408.33 T
229.29 689.78 229.29 692.18 2 L
7 X
V
1 H
2 Z
0 X
N
228.09 690.98 230.49 690.98 2 L
7 X
V
0 X
N
219.72 684.31 219.72 686.71 2 L
7 X
V
0 X
N
218.52 685.51 220.92 685.51 2 L
7 X
V
0 X
N
210.15 684.31 210.15 686.71 2 L
7 X
V
0 X
N
208.95 685.51 211.35 685.51 2 L
7 X
V
0 X
N
200.57 680.46 200.57 682.86 2 L
7 X
V
0 X
N
199.38 681.66 201.78 681.66 2 L
7 X
V
0 X
N
191 661.55 191 663.95 2 L
7 X
V
0 X
N
189.8 662.75 192.2 662.75 2 L
7 X
V
0 X
N
181.43 643.66 181.43 646.07 2 L
7 X
V
0 X
N
180.23 644.86 182.63 644.86 2 L
7 X
V
0 X
N
171.86 620.84 171.86 623.24 2 L
7 X
V
0 X
N
170.66 622.04 173.06 622.04 2 L
7 X
V
0 X
N
162.29 583.19 162.29 585.59 2 L
7 X
V
0 X
N
161.09 584.39 163.49 584.39 2 L
7 X
V
0 X
N
152.71 563.1 152.71 565.5 2 L
7 X
V
0 X
N
151.51 564.3 153.91 564.3 2 L
7 X
V
0 X
N
143.14 552.57 143.14 554.97 2 L
7 X
V
0 X
N
141.94 553.77 144.34 553.77 2 L
7 X
V
0 X
N
133.57 547.42 133.57 549.82 2 L
7 X
V
0 X
N
132.37 548.61 134.77 548.61 2 L
7 X
V
0 X
N
124 544.52 124 546.92 2 L
7 X
V
0 X
N
122.8 545.72 125.2 545.72 2 L
7 X
V
0 X
N
124 546.33 133.57 549.28 143.14 554.86 152.71 564.85 162.29 581.21 171.86 604.37 181.43 631.18
 191 655.9 200.57 674.32 210.15 685.99 219.72 692.64 229.29 696.21 12 L
9 X
N
124 543.26 251.19 543.26 2 L
2 H
0 X
N
124 543.26 124 706.05 2 L
N
124 543.26 124 541.66 2 L
N
0 12 Q
(1) 121 529.83 T
155.8 543.26 155.8 541.66 2 L
N
(10) 149.8 529.83 T
187.6 543.26 187.6 541.66 2 L
N
(100) 178.6 529.83 T
219.39 543.26 219.39 541.66 2 L
N
(1000) 207.4 529.83 T
251.19 543.26 251.19 541.66 2 L
N
(10000) 236.2 529.83 T
124 543.26 122.4 543.26 2 L
N
(0) 113.4 539.43 T
124 583.96 122.4 583.96 2 L
N
(10) 110.4 580.13 T
124 624.65 122.4 624.65 2 L
N
(20) 110.4 620.82 T
124 665.35 122.4 665.35 2 L
N
(30) 110.4 661.52 T
124 706.05 122.4 706.05 2 L
N
(40) 110.4 702.22 T
0 14 Q
(HIPPI Request Size \050KB\051) 115.89 517.14 T
(Throughput \050MB/s\051) 0 -270 102.4 569.66 TF
0 0 612 792 C
70.21 72 298.79 395.6 C
74.88 76.1 295.38 189.8 R
7 X
0 K
V
2 11 Q
0 X
1.76 (Figur) 74.88 183.13 P
1.76 (e 4) 100.93 183.13 P
2 10 Q
1.61 (: Parity Engine Performance.) 115.81 183.13 P
0 F
1.61 ( This \336gure) 246.09 183.13 P
4 (shows the performance of the parity computation) 74.88 173.13 P
-0.17 (engine. The symbols represent measured data, gathered) 74.88 163.13 P
0.68 (from workloads with buf) 74.88 153.13 P
0.68 (fer sizes that are powers of 2) 176.4 153.13 P
0.25 (between 1 KB and 1 MB, and {1, 2, 4, 8, and 16} data) 74.88 143.13 P
0.65 (buf) 74.88 133.13 P
0.65 (fers. The line represents performance derived from) 88.02 133.13 P
0.04 (a simple model with a constant overhead of 0.9 ms and) 74.88 123.13 P
3.16 (a maximum throughput of 38.5 MB/s. Throughput) 74.88 113.13 P
5.85 (measures the total number of bytes transferred) 74.88 103.13 P
2.35 (\050including both data and parity\051 divided by elapsed) 74.88 93.13 P
(time.) 74.88 83.13 T
0.5 H
0 Z
90 450 0.28 0.28 228.22 373.28 A
90 450 0.28 0.28 218.64 368.84 A
90 450 0.28 0.28 209.07 362.46 A
90 450 0.28 0.28 199.5 350.29 A
90 450 0.28 0.28 189.93 331.1 A
90 450 0.28 0.28 180.35 307.04 A
90 450 0.28 0.28 170.78 280.68 A
90 450 0.28 0.28 229.01 374.55 A
90 450 0.28 0.28 219.43 371.55 A
90 450 0.28 0.28 209.86 365.06 A
90 450 0.28 0.28 200.29 352.75 A
90 450 0.28 0.28 190.72 334.14 A
90 450 0.28 0.28 181.14 312.27 A
90 450 0.28 0.28 171.57 286.28 A
90 450 0.28 0.28 162 258.17 A
90 450 0.28 0.28 230.46 377.04 A
90 450 0.28 0.28 220.89 373.36 A
90 450 0.28 0.28 211.32 367.99 A
90 450 0.28 0.28 201.74 357.9 A
90 450 0.28 0.28 192.17 341.64 A
90 450 0.28 0.28 182.6 317.18 A
90 450 0.28 0.28 173.03 290.58 A
90 450 0.28 0.28 163.45 263.06 A
90 450 0.28 0.28 153.88 246.27 A
90 450 0.28 0.28 232.98 379.9 A
90 450 0.28 0.28 223.41 377.43 A
90 450 0.28 0.28 213.83 372.23 A
90 450 0.28 0.28 204.26 364.06 A
90 450 0.28 0.28 194.69 348.36 A
90 450 0.28 0.28 185.12 327.36 A
90 450 0.28 0.28 175.54 300.44 A
90 450 0.28 0.28 165.97 270.68 A
90 450 0.28 0.28 156.4 247.47 A
90 450 0.28 0.28 146.83 237.1 A
90 450 0.28 0.28 236.95 385.78 A
90 450 0.28 0.28 227.38 383.14 A
90 450 0.28 0.28 217.81 379.34 A
90 450 0.28 0.28 208.23 371.38 A
90 450 0.28 0.28 198.66 358.42 A
90 450 0.28 0.28 189.09 340.14 A
90 450 0.28 0.28 179.52 314.35 A
90 450 0.28 0.28 169.94 284.09 A
90 450 0.28 0.28 160.37 254.92 A
90 450 0.28 0.28 150.8 242.6 A
90 450 0.28 0.28 141.23 233.41 A
131.65 229.91 131.65 230.71 2 L
1 H
2 Z
N
131.25 230.3 132.05 230.3 2 L
N
141.23 234.28 141.23 235.08 2 L
N
140.83 234.68 141.63 234.68 2 L
N
150.8 242.32 150.8 243.12 2 L
N
150.4 242.72 151.2 242.72 2 L
N
160.37 256.04 160.37 256.84 2 L
N
159.97 256.44 160.77 256.44 2 L
N
169.94 276.71 169.94 277.51 2 L
N
169.54 277.11 170.34 277.11 2 L
N
179.52 302.72 179.52 303.52 2 L
N
179.12 303.12 179.92 303.12 2 L
N
189.09 328.93 189.09 329.73 2 L
N
188.69 329.33 189.49 329.33 2 L
N
198.66 350.04 198.66 350.84 2 L
N
198.26 350.44 199.06 350.44 2 L
N
208.23 364.18 208.23 364.98 2 L
N
207.83 364.58 208.63 364.58 2 L
N
217.81 372.53 217.81 373.33 2 L
N
217.41 372.93 218.21 372.93 2 L
N
227.38 377.09 227.38 377.89 2 L
N
226.98 377.49 227.78 377.49 2 L
N
236.95 379.48 236.95 380.28 2 L
N
236.55 379.88 237.35 379.88 2 L
N
131.65 230.3 141.23 234.68 150.8 242.72 160.37 256.44 169.94 277.11 179.52 303.12 189.09 329.33
 198.66 350.44 208.23 364.58 217.81 372.93 227.38 377.49 236.95 379.88 12 L
N
131.65 225.66 258.85 225.66 2 L
2 H
N
131.65 225.66 131.65 388.45 2 L
N
131.65 225.66 131.65 224.06 2 L
N
0 12 Q
(1) 128.66 213.03 T
163.45 225.66 163.45 224.06 2 L
N
(10) 157.46 213.03 T
195.25 225.66 195.25 224.06 2 L
N
(100) 186.26 213.03 T
227.05 225.66 227.05 224.06 2 L
N
(1000) 215.06 213.03 T
258.85 225.66 258.85 224.06 2 L
N
(10000) 243.86 213.03 T
131.65 225.66 130.05 225.66 2 L
N
(0) 117.86 221.83 T
131.65 266.36 130.05 266.36 2 L
N
(10) 114.86 262.53 T
131.65 307.05 130.05 307.05 2 L
N
(20) 114.86 303.22 T
131.65 347.75 130.05 347.75 2 L
N
(30) 114.86 343.92 T
131.65 388.45 130.05 388.45 2 L
N
(40) 114.86 384.62 T
0 14 Q
(KB T) 102.62 196.34 T
(ransferred by Parity Engine) 133.62 196.34 T
(Throughput \050MB/s\051) 0 -270 111.65 252.06 TF
0 0 612 792 C
0 10 Q
0 X
0 K
0.62 (rather than the number of data buf) 315 713.33 P
0.62 (fers. The line in Fig-) 455.08 713.33 P
2.61 (ure 4 represents performance derived from a simple) 315 703.33 P
0.82 (model with a constant overhead of 0.9 ms and a maxi-) 315 693.33 P
0.75 (mum throughput of 38.5 MB/s. As with the HIPPI, the) 315 683.33 P
0.39 (parity engine is not a bottleneck, though the \336xed over-) 315 673.33 P
1.72 (head of 0.9 ms degrades performance when less than) 315 663.33 P
(100 KB is transferred.) 315 653.33 T
2 12 Q
(3.4  Disk Subsystem) 315 636 T
0 10 Q
-0.19 (Figure 5 and Figure 6 show how well RAID-II per-) 336.6 625.33 P
1.51 (formance scales with the number of disks. All graphs) 315 615.33 P
0.88 (use 64 KB requests. The dashed lines indicate the per-) 315 605.33 P
1.89 (formance under perfect, linear scaling. Figure 5a and) 315 595.33 P
1.16 (Figure 6a show how the performance of one string on) 315 585.33 P
-0.19 (one Interphase Cougar disk controller scales as the num-) 315 575.33 P
0.52 (ber of disks varies from 1 to 5. Each string can support) 315 565.33 P
1.9 (the full bandwidth of two disks; with more than two) 315 555.33 P
0.36 (disks, the maximum string throughput of 3.1 MB/s lim-) 315 545.33 P
0.11 (its performance. For all other graphs, we use three disks) 315 535.33 P
(per string.) 315 525.33 T
0.05 (Figure 5b and Figure 6b show how well each Cou-) 336.6 515.33 P
2 (gar disk controller supports multiple strings. In these) 315 505.33 P
0.82 (\336gures, we graph how performance increases as we go) 315 495.33 P
0.4 (from one string per controller to two. W) 315 485.33 P
0.4 (e see that using) 477.45 485.33 P
1.41 (two strings per controller almost doubles performance) 315 475.33 P
1.02 (over using one string per controller) 315 465.33 P
1.02 (, topping out at 5.3) 460.42 465.33 P
1.01 (MB/s for disk reads and 5.6 MB/s for disk writes. For) 315 455.33 P
(the rest of the graphs, we use two strings per controller) 315 445.33 T
(.) 534.01 445.33 T
1.1 (Figure 5c and Figure 6c vary the number of disk) 336.6 435.33 P
0.4 (controllers per VME bus from one to two. Performance) 315 425.33 P
0.67 (per VME bus is limited to 6.9 MB/s for disk reads and) 315 415.33 P
1.21 (5.9 MB/s for disk writes due to a relatively slow syn-) 315 405.33 P
-0.23 (chronous design of the XBUS VME interface [Katz93]) 315 395.33 P
0 8 Q
-0.18 (5) 533.5 399.33 P
0 10 Q
-0.23 (.) 537.5 395.33 P
0.09 (For the remainder of the paper) 315 385.33 P
0.09 (, we use one disk control-) 436.87 385.33 P
(ler per VME bus.) 315 375.33 T
1.62 (Figure 5d and Figure 6d increase the number of) 336.6 365.33 P
-0.08 (VME busses active on the XBUS from one to \336ve) 315 355.33 P
0 8 Q
-0.06 (6) 514.43 359.33 P
0 10 Q
-0.08 (. Per-) 518.43 355.33 P
2.31 (formance of the XBUS card scales linearly with the) 315 345.33 P
(number of controllers for both reads and writes.) 315 335.33 T
-0.01 (It is clear from these graphs that the limiting factor) 336.6 325.33 P
0.4 (to overall system performance is the disk subsystem. In) 315 315.33 P
-0.22 (designing RAID-II, we expected each string to support 4) 315 305.33 P
1.05 (MB/s. Thus, with four dual-string controllers, the total) 315 295.33 P
0.67 (RAID-II disk bandwidth should be 32 MB/s. However) 315 285.33 P
0.67 (,) 537.5 285.33 P
0.05 (with three disks per string, each string transfers only 3.1) 315 275.33 P
0.04 (MB/s for disk writes and 2.6 MB/s for disk reads, so the) 315 265.33 P
1.82 (maximum total string bandwidth is 25 MB/s for disk) 315 255.33 P
-0.18 (writes and 21 MB/s for disk reads. Other than string per-) 315 245.33 P
0.47 (formance, the system scales linearly with one disk con-) 315 235.33 P
(troller on each VME bus.) 315 225.33 T
2 12 Q
(3.5  Evaluation of Design) 315 208 T
0 10 Q
0.96 (In this section, we examine the four design ques-) 336.6 197.33 P
(tions raised in Section 2:) 315 187.33 T
315 152 540 172 C
315 160 447 160 2 L
0.25 H
2 Z
0 X
0 K
N
0 0 612 792 C
0 10 Q
0 X
0 K
-0.05 (5.  T) 315 145.33 P
-0.05 (o implement the VME interface as quickly as possi-) 332.79 145.33 P
-0.08 (ble, we designed a simple, synchronous VME interface.) 315 135.33 P
(This interface takes approximately 5 XBUS cycles per) 315 125.33 T
-0.12 (VME word. By pipelining data words and using a faster) 315 115.33 P
(VME clock rate, this interface could be sped up signi\336-) 315 105.33 T
(cantly) 315 95.33 T
(.) 338.78 95.33 T
(6.  The disk controller on the \336fth VME bus used only) 315 85.33 T
(one string due to lack of suf) 315 75.33 T
(\336cient cabling.) 426.41 75.33 T
FMENDPAGE
%%EndPage: "6" 5
%%Page: "5" 5
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
0 10 Q
1.1 (W) 72 713.33 P
1.1 (e measure the performance of this parity engine and) 80.64 713.33 P
1.27 (compare it to the performance of computing parity on) 72 703.33 P
(the workstation \336le server) 72 693.33 T
(.) 175.54 693.33 T
1 F
0.28 (The fourth design issue this paper addr) 93.6 683.33 P
0.28 (esses is the) 252.03 683.33 P
-0.16 (success or failur) 72 673.33 P
-0.16 (e of connecting the network and disks to) 137.38 673.33 P
(the XBUS instead of to the \336le server) 72 663.33 T
(.) 218.85 663.33 T
0 F
-0.04 (A key feature of RAID-II\325) 93.6 653.33 P
-0.04 (s architecture is the min-) 198.34 653.33 P
0.47 (imal use of the \336le server) 72 643.33 P
0.47 (\325) 176.05 643.33 P
0.47 (s backplane. Data never goes) 178.82 643.33 P
2.27 (over the \336le server) 72 633.33 P
2.27 (\325) 153.85 633.33 P
2.27 (s backplane; instead, data travels) 156.62 633.33 P
0.64 (directly from the HIPPI network to the XBUS memory) 72 623.33 P
0.04 (to the disks. The \336le server still controls this data move-) 72 613.33 P
2.07 (ment but never touches the data.) 72 603.33 P
0 8 Q
1.66 (3) 211.7 607.33 P
0 10 Q
2.07 ( By doing this, we) 215.7 603.33 P
0.86 (hope to achieve end-to-end performance that is limited) 72 593.33 P
-0.12 (by XBUS\325) 72 583.33 P
-0.12 (s performance rather than the \336le server back-) 113.81 583.33 P
(plane\325) 72 573.33 T
(s performance.) 96.43 573.33 T
2 12 Q
(3 Performance and Design Evaluation) 72 556 T
0 10 Q
1.96 (This section is or) 93.6 545.33 P
1.96 (ganized in two parts. The \336rst) 167.86 545.33 P
1.1 (part reports a series of system-level performance mea-) 72 535.33 P
2.76 (surements by sending data from the disk subsystem) 72 525.33 P
1.81 (to/from the network and analyzes the performance of) 72 515.33 P
2.01 (each system component to locate system bottlenecks.) 72 505.33 P
1.82 (The second part examines the design issues raised in) 72 495.33 P
(Section 2.) 72 485.33 T
2 12 Q
(3.1  System Performance) 72 468 T
0 10 Q
1 (For all system-level experiments, the disk system) 93.6 457.33 P
0.23 (is con\336gured as a RAID Level 5 [Patterson88] with one) 72 447.33 P
1.28 (parity group of 24 disks. W) 72 437.33 P
1.28 (e break down system tests) 187.52 437.33 P
0.58 (into two separate experiments, reads and writes; Figure) 72 427.33 P
0.04 (2 shows the results. For reads, data is read from the disk) 72 417.33 P
-0.15 (array into the memory on the XBUS card and from there) 72 407.33 P
0.96 (is sent over HIPPI back to the XBUS card into XBUS) 72 397.33 P
0.84 (memory) 72 387.33 P
0.84 (. For writes, data originates in XBUS memory) 104.66 387.33 P
0.84 (,) 294.5 387.33 P
0.24 (is sent over the HIPPI back to the XBUS card to XBUS) 72 377.33 P
1.07 (memory) 72 367.33 P
1.07 (, parity is computed, then both data and parity) 104.66 367.33 P
0.35 (are written to the disk array) 72 357.33 P
0.35 (. W) 183.29 357.33 P
0.35 (e use the XBUS card for) 197.27 357.33 P
0.42 (both sending and receiving the data because of our cur-) 72 347.33 P
0.52 (rent lack of another system that can source or sink data) 72 337.33 P
0.77 (at the necessary bandwidth.) 72 327.33 P
0 8 Q
0.61 (4) 184.77 331.33 P
0 10 Q
0.77 ( For both tests, the system) 188.77 327.33 P
0.62 (is con\336gured with four Interphase Cougar disk control-) 72 317.33 P
2.36 (lers with six disks on each disk controller) 72 307.33 P
2.36 (. Figure 2) 253.69 307.33 P
2.31 (shows that, for lar) 72 297.33 P
2.31 (ge requests, system-level read and) 150.92 297.33 P
-0.06 (write performance tops out at about 20 MB/s. W) 72 287.33 P
-0.06 (rites are) 265.15 287.33 P
0.11 (slower than reads due to the increased disk and memory) 72 277.33 P
1.66 (activity associated with computing and writing parity) 72 267.33 P
1.66 (.) 294.5 267.33 P
1.55 (While an order of magnitude faster than our previous) 72 257.33 P
0.68 (prototype, RAID-I, this is still well below our tar) 72 247.33 P
0.68 (get of) 273.28 247.33 P
1.16 (40 MB/s. In the next sections, we measure the perfor-) 72 237.33 P
0.23 (mance of each component to determine what limits per-) 72 227.33 P
(formance.) 72 217.33 T
2 12 Q
(3.2  HIPPI) 72 200 T
0 10 Q
0.44 (Figure 3 shows the performance of the HIPPI net-) 93.6 189.33 P
1.72 (work and boards. Data is transferred from the XBUS) 72 179.33 P
-0.14 (memory to the HIPPI source board to the HIPPI destina-) 72 169.33 P
0.03 (tion board and back to XBUS memory) 72 159.33 P
0.03 (. Because the net-) 225.86 159.33 P
0.76 (work is con\336gured as a loop, there is minimal network) 72 149.33 P
1.69 (protocol overhead\321this test focuses on the network\325) 72 139.33 P
1.69 (s) 293.11 139.33 P
72 112 297 132 C
72 120 204 120 2 L
0.25 H
2 Z
0 X
0 K
N
0 0 612 792 C
0 10 Q
0 X
0 K
(3.  The server does store and manipulate the meta-data,) 72 105.33 T
(such as inodes and directory structures.) 72 95.33 T
(4.  W) 72 85.33 T
(e are in the process of connecting to a 100 MB/s) 93.13 85.33 T
(video microscope at Lawrence Berkeley Laboratories.) 72 75.33 T
0.9 (raw hardware performance. The overhead of sending a) 315 713.33 P
0.73 (HIPPI packet is about 1.1 ms, mostly due to setting up) 315 703.33 P
1.06 (the HIPPI and XBUS control registers across the slow) 315 693.33 P
2.8 (VME link \050in comparison, an Ethernet packet takes) 315 683.33 P
1.48 (approximately 0.5 ms to transfer\051. Due to this control) 315 673.33 P
0.3 (overhead, small requests result in low performance. For) 315 663.33 P
1.57 (lar) 315 653.33 P
1.57 (ge requests, however) 325.36 653.33 P
1.57 (, the XBUS and HIPPI boards) 412.21 653.33 P
0.38 (support 38 MB/s in both directions, which is very close) 315 643.33 P
0.26 (to the maximum bandwidth of each XBUS port. During) 315 633.33 P
1.75 (these lar) 315 623.33 P
1.75 (ge transfers, the XBUS card is transferring a) 350.15 623.33 P
0.49 (total of 76 MB/s, which is an order of magnitude faster) 315 613.33 P
2.52 (than FDDI and two orders of magnitude faster than) 315 603.33 P
0.26 (Ethernet. Clearly the HIPPI part of the XBUS is not the) 315 593.33 P
(liming factor in determining system level performance.) 315 583.33 T
2 12 Q
(3.3  Parity Engine) 315 566 T
0 10 Q
0.03 (Figure 4 shows the performance of the parity com-) 336.6 555.33 P
1.09 (putation engine, which is used in performing writes to) 315 545.33 P
0.4 (the disk array) 315 535.33 P
0.4 (. After the \336le server writes control words) 369.55 535.33 P
1.44 (into the parity engine\325) 315 525.33 P
1.44 (s DMA, the parity engine reads) 407.33 525.33 P
0.38 (words from XBUS memory) 315 515.33 P
0.38 (, computes the exclusive-or) 426.83 515.33 P
0.38 (,) 537.5 515.33 P
2.24 (and then writes the resulting parity blocks back into) 315 505.33 P
2.26 (XBUS memory) 315 495.33 P
2.26 (. Figure 4 graphs performance for all) 379.07 495.33 P
3.72 (combinations of buf) 315 485.33 P
3.72 (fer sizes that are powers of 2) 402.77 485.33 P
0.79 (between 1 KB and 1 MB and {1, 2, 4, 8, and 16} data) 315 475.33 P
1.88 (buf) 315 465.33 P
1.88 (fers. The amount of data transferred by the parity) 328.14 465.33 P
1.53 (engine is the size of each buf) 315 455.33 P
1.53 (fer times the number of) 440.04 455.33 P
2.54 (buf) 315 445.33 P
2.54 (fers plus one \050for the result buf) 328.14 445.33 P
2.54 (fer\051. Performance) 466.98 445.33 P
0.78 (depends mainly on the total amount of data transferred) 315 435.33 P
314.2 72 540.8 423 C
313.55 76.6 533.8 220.6 R
7 X
0 K
V
2 11 Q
0 X
5.85 (Figur) 313.55 213.93 P
5.85 (e 2: System Level Read and W) 339.6 213.93 P
5.85 (rite) 517.33 213.93 P
8.35 (Performance.) 313.55 203.93 P
0 10 Q
7.59 ( This \336gure shows the overall) 377.3 203.93 P
2.24 (performance of the RAID-II storage server for disk) 313.55 193.93 P
0.65 (reads and writes. For disk reads, data is read from the) 313.55 183.93 P
2.13 (disk array into XBUS memory) 313.55 173.93 P
2.13 (, transferred over the) 444.14 173.93 P
1.14 (HIPPI network and back to XBUS memory) 313.55 163.93 P
1.14 (. For disk) 493.49 163.93 P
-0.22 (writes, data starts in XBUS memory) 313.55 153.93 P
-0.22 (, is transferred over) 456.73 153.93 P
1.48 (the HIPPI network back to XBUS memory) 313.55 143.93 P
1.48 (, parity is) 493.35 143.93 P
1.28 (computed, then the data and parity are written to the) 313.55 133.93 P
3.55 (disk array) 313.55 123.93 P
3.55 (. CPU utilization limits performance for) 356.14 123.93 P
7.9 (small requests; disk system throughput limits) 313.55 113.93 P
1.04 (performance for lar) 313.55 103.93 P
1.04 (ge requests. Request size refers to) 393.15 103.93 P
1.93 (the total size of the request before it is broken into) 313.55 93.93 P
(individual disk requests.) 313.55 83.93 T
352.77 270.53 352.77 272.93 2 L
7 X
V
1 H
2 Z
0 X
N
351.57 271.73 353.96 271.73 2 L
7 X
V
0 X
N
356.83 281.36 356.83 283.76 2 L
7 X
V
0 X
N
355.64 282.56 358.04 282.56 2 L
7 X
V
0 X
N
364.98 296.09 364.98 298.49 2 L
7 X
V
0 X
N
363.77 297.29 366.18 297.29 2 L
7 X
V
0 X
N
381.26 300.81 381.26 303.21 2 L
7 X
V
0 X
N
380.06 302.01 382.46 302.01 2 L
7 X
V
0 X
N
397.54 313.26 397.54 315.66 2 L
7 X
V
0 X
N
396.34 314.46 398.74 314.46 2 L
7 X
V
0 X
N
413.82 329.07 413.82 331.47 2 L
7 X
V
0 X
N
412.62 330.27 415.02 330.27 2 L
7 X
V
0 X
N
430.1 344.2 430.1 346.6 2 L
7 X
V
0 X
N
428.9 345.4 431.3 345.4 2 L
7 X
V
0 X
N
442.31 351.56 442.31 353.96 2 L
7 X
V
0 X
N
441.11 352.76 443.51 352.76 2 L
7 X
V
0 X
N
352.77 271.73 356.83 282.56 364.98 297.29 381.26 302.01 397.54 314.46 413.82 330.27 430.1 345.4
 442.31 352.76 8 L
N
352.77 290.3 352.77 292.7 2 L
N
351.57 291.5 353.96 291.5 2 L
N
356.83 308.09 356.83 310.49 2 L
N
355.64 309.29 358.04 309.29 2 L
N
364.98 334.13 364.98 336.53 2 L
N
363.77 335.33 366.18 335.33 2 L
N
381.26 353.44 381.26 355.84 2 L
N
380.06 354.64 382.46 354.64 2 L
N
397.54 343.96 397.54 346.36 2 L
N
396.34 345.16 398.74 345.16 2 L
N
413.82 355.08 413.82 357.48 2 L
N
412.62 356.29 415.02 356.29 2 L
N
430.1 357.35 430.1 359.75 2 L
N
428.9 358.55 431.3 358.55 2 L
N
446.38 366.1 446.38 368.5 2 L
N
445.18 367.3 447.58 367.3 2 L
N
352.77 291.5 356.83 309.29 364.98 335.33 381.26 354.64 397.54 345.16 413.82 356.29 430.1 358.55
 446.38 367.3 8 L
N
348.7 254.04 475.89 254.04 2 L
2 H
N
348.7 254.04 348.7 416.83 2 L
N
348.7 254.04 348.7 252.44 2 L
N
0 12 Q
(0) 345.7 241.37 T
380.49 254.04 380.49 252.44 2 L
N
(500) 371.5 241.37 T
412.29 254.04 412.29 252.44 2 L
N
(1000) 400.3 241.37 T
444.09 254.04 444.09 252.44 2 L
N
(1500) 432.1 241.37 T
475.89 254.04 475.89 252.44 2 L
N
(2000) 463.89 241.37 T
348.7 254.04 347.1 254.04 2 L
N
(0) 334.7 250.77 T
348.7 308.31 347.1 308.31 2 L
N
(10) 331.7 305.03 T
348.7 362.57 347.1 362.57 2 L
N
(20) 331.7 359.3 T
348.7 416.83 347.1 416.83 2 L
N
(30) 331.7 413.56 T
0 14 Q
(Request Size \050KB\051) 359.83 230.49 T
(Throughput \050MB/s\051) 0 -270 326.1 280.44 TF
0 12 Q
(Disk Array Reads) 449.49 364.76 T
(Disk Array W) 445.89 350.36 T
(rites) 513.35 350.36 T
0 0 612 792 C
FMENDPAGE
%%EndPage: "5" 4
%%Page: "4" 4
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
0 10 Q
0.79 (transfer bursts of data at 50 MB/s and sustain transfers) 72 713.33 P
1.16 (at 40 MB/s, for a total sustainable memory bandwidth) 72 703.33 P
(on the XBUS card of 160 MB/s.) 72 693.33 T
7.87 (The XBUS is a synchronous multiplexed) 93.6 683.33 P
2.24 (\050address/data\051 crossbar) 72 673.33 P
2.24 (-based interconnect that uses a) 165.9 673.33 P
1.14 (centralized strict priority-based arbitration scheme. All) 72 663.33 P
2.1 (paths to memory can be recon\336gured on a cycle-by-) 72 653.33 P
0.76 (cycle basis. Each of the eight 32-bit XBUS ports oper-) 72 643.33 P
(ates at a cycle time of 80 ns.) 72 633.33 T
1.01 (The XBUS supports reads and write transactions.) 93.6 623.33 P
0.17 (Between 1 and 16 words are transferred over the XBUS) 72 613.33 P
0.5 (during each transaction. Each transaction consists of an) 72 603.33 P
0.39 (arbitration phase, an address phase, and a data phase. If) 72 593.33 P
1.38 (there is no contention for memory) 72 583.33 P
1.38 (, the arbitration and) 214.57 583.33 P
2.03 (address phases each take a single cycle; data is then) 72 573.33 P
0.38 (transferred at the rate of one word per cycle. The mem-) 72 563.33 P
0.23 (ory may arbitrarily insert wait cycles during the address) 72 553.33 P
0.51 (and data cycles to compensate for DRAM access laten-) 72 543.33 P
1.03 (cies and refreshes. The shortest XBUS transaction is a) 72 533.33 P
0.58 (single word write, taking three cycles \050one each for the) 72 523.33 P
(arbitration, address, and data phases\051.) 72 513.33 T
1.9 (Data is interleaved across the four banks in 16-) 93.6 503.33 P
0.02 (word interleave units. Although the crossbar is designed) 72 493.33 P
1.93 (to move lar) 72 483.33 P
1.93 (ge blocks between memory) 121.2 483.33 P
1.93 (, network, and) 235.97 483.33 P
-0.01 (disk interfaces, it is still possible to access a single word) 72 473.33 P
0.77 (when necessary) 72 463.33 P
0.77 (. The external \336le server can access the) 135.11 463.33 P
0.62 (on-board memory through the XBUS card\325) 72 453.33 P
0.62 (s VME con-) 246.91 453.33 P
(trol interface.) 72 443.33 T
1.45 (Of the eight client XBUS ports, two interface to) 93.6 433.33 P
0.73 (the TMC I/O bus \050HIPPIS/HIPPID busses\051. The HIPPI) 72 423.33 P
0.51 (board set also interfaces to this bus. These XBUS ports) 72 413.33 P
0.91 (are unidirectional and can sustain transfers of up to 40) 72 403.33 P
0.62 (MB/s, with bursts of up to 100 MB/s into 32 KB FIFO) 72 393.33 P
(interfaces.) 72 383.33 T
1.97 (Four of the client ports are used to connect the) 93.6 373.33 P
-0.18 (XBUS card to four VME busses, each of which can con-) 72 363.33 P
0.54 (nect to one or two dual-string disk controllers. Because) 72 353.33 P
0.23 (of the physical packaging of the array) 72 343.33 P
0.23 (, 6 to 12 disks can) 223.69 343.33 P
0.52 (be attached to each disk controller in two rows of three) 72 333.33 P
0.15 (to six disks each. Thus, up to 96 disk drives can be con-) 72 323.33 P
1.01 (nected to each XBUS card. Each VME interface has a) 72 313.33 P
0.95 (maximum transfer bandwidth of 40 MB/s; however) 72 303.33 P
0.95 (, in) 283.28 303.33 P
1.32 (our experience, the Sun 4 VME can usually realize at) 72 293.33 P
-0.14 (most 8-10 MB/s [Chervenak91]. The VME disk control-) 72 283.33 P
0.16 (lers that we use, Interphase Cougar disk controllers, can) 72 273.33 P
-0.24 (transfer data at 8 MB/s) 72 263.33 P
0 8 Q
-0.19 (1) 162.63 267.33 P
0 10 Q
-0.24 (, for a total maximum bandwidth) 166.63 263.33 P
(to the disk array of 32 MB/s.) 72 253.33 T
-0.08 (Of the remaining two client ports, one interfaces to) 93.6 243.33 P
2.62 (a parity computation engine. The last port links the) 72 233.33 P
0.14 (XBUS card to the external \336le server) 72 223.33 P
0.14 (. It provides access) 220.52 223.33 P
1.03 (to the on-board memory as well as the board\325) 72 213.33 P
1.03 (s control) 261.26 213.33 P
-0.22 (registers \050through the board\325) 72 203.33 P
-0.22 (s control bus\051. This makes it) 184.85 203.33 P
0.19 (possible for \336le server software, running on the control-) 72 193.33 P
0.74 (ler) 72 183.33 P
0.74 (, to access network headers and \336le meta-data in the) 82.14 183.33 P
(controller cache.) 72 173.33 T
2 12 Q
(2.2  Design Issues) 72 156 T
1 10 Q
2.25 (The \336rst design issue is whether memory/XBUS) 93.6 145.33 P
-0.14 (contention signi\336cantly degrades system performance in) 72 135.33 P
(the XBUS design.) 72 125.33 T
72 102 297 122 C
72 110 204 110 2 L
0.25 H
2 Z
0 X
0 K
N
0 0 612 792 C
0 10 Q
0 X
0 K
-0.24 (1.  This is the transfer rate for SCSI-1 drives; with SCSI-) 72 95.33 P
(2, the Cougar disk controllers can transfer at higher) 72 85.33 T
(rates.) 72 75.33 T
1.75 (Before deciding upon the XBUS, we considered) 336.6 713.33 P
-0.05 (using a single, wide bus interconnect. Its main attraction) 315 703.33 P
0.21 (was conceptual simplicity) 315 693.33 P
0.21 (. However) 418.6 693.33 P
0.21 (, a 128-bit wide bus) 460.04 693.33 P
1.41 (would have required a huge number of FIFO and bus) 315 683.33 P
2 (transceiver chips. While we could have used a small) 315 673.33 P
0.53 (number of time-multiplexed 128-bit ports, interfaced to) 315 663.33 P
-0.01 (narrower 32-bit and 64-bit busses, the result would have) 315 653.33 P
(been a more complex system.) 315 643.33 T
1.98 (A disadvantage of using a crossbar interconnect) 336.6 633.33 P
1.27 (with multiple, independent memories is the possibility) 315 623.33 P
1.49 (of contention for memory) 315 613.33 P
1.49 (. For example, if there were) 421.81 613.33 P
0.79 (four client ports \050each transferring at 40 MB/s\051 access-) 315 603.33 P
1.63 (ing a random memory) 315 593.33 P
1.63 (, then on average 1.26 memory) 407.79 593.33 P
0.4 (ports would be idle and 50 MB/s of the available mem-) 315 583.33 P
2 (ory bandwidth would be wasted. The original XBUS) 315 573.33 P
0.16 (design had eight memory ports to minimize this conten-) 315 563.33 P
0.59 (tion; however) 315 553.33 P
0.59 (, we later reduced the number of memory) 370.43 553.33 P
0.05 (ports to four due to problems routing the original design) 315 543.33 P
1.75 ([Katz93].) 315 533.33 P
0 8 Q
1.4 (2) 353.02 537.33 P
0 10 Q
1.75 ( The total sustainable demand of all client-) 357.02 533.33 P
-0.04 (ports is 150 MB/s \05040 MB/s for each non-VME port and) 315 523.33 P
0.42 (6 MB/s for each VME\051, so the XBUS could be close to) 315 513.33 P
2.15 (fully utilized and memory contention could seriously) 315 503.33 P
(degrade system performance.) 315 493.33 T
1.65 (While contention for memory modules is a con-) 336.6 483.33 P
4.01 (cern, actual contention should be infrequent. Most) 315 473.33 P
0.37 (XBUS ports perform lar) 315 463.33 P
0.37 (ge accesses at least of at least a) 412.8 463.33 P
0.43 (few KB so that when two accesses con\337ict, the loser of) 315 453.33 P
4.3 (the arbitration deterministically follows the winner) 315 443.33 P
0.3 (around the memory modules, avoiding further con\337icts.) 315 433.33 P
1.36 (Each XBUS port buf) 315 423.33 P
1.36 (fers 4-32 KB of data to/from the) 402.47 423.33 P
(XBUS to even out \337uctuations.) 315 413.33 T
1 F
-0.07 (The second design issue is the necessity of two net-) 336.6 403.33 P
(works, one for high bandwidth, one for low latency) 315 393.33 T
(.) 518.79 393.33 T
0 F
1.82 (The remote connection of the HIPPI network to) 336.6 383.33 P
-0.01 (the XBUS card increases the latency of sending a HIPPI) 315 373.33 P
2.04 (packet. Thus, while the XBUS\325) 315 363.33 P
2.04 (s HIPPI network pro-) 448.1 363.33 P
0.47 (vides high bandwidth, its latency is actually worse than) 315 353.33 P
1.15 (if we had directly connected it to the Sun4 \336le server) 315 343.33 P
1.15 (.) 537.5 343.33 P
2.92 (Also, because it is a high-bandwidth network, even) 315 333.33 P
2.06 (small latencies waste a lar) 315 323.33 P
2.06 (ge fraction of its available) 427.41 323.33 P
0.46 (bandwidth and it is thus inef) 315 313.33 P
0.46 (\336cient at transferring small) 430.65 313.33 P
0.09 (packets. Thus, RAID-II also supports an Ethernet that is) 315 303.33 P
2.87 (directly connected to the Sun4 \336le server for small) 315 293.33 P
-0.02 (transfers where network latency dominates service time.) 315 283.33 P
0.58 (In this paper) 315 273.33 P
0.58 (, we measure the latency of the high band-) 365.73 273.33 P
2.52 (width HIPPI to determine if having the low latency) 315 263.33 P
(Ethernet is necessary) 315 253.33 T
(.) 398.72 253.33 T
1 F
-0.14 (The thir) 336.6 243.33 P
-0.14 (d design issue is the performance bene\336t of) 368.02 243.33 P
(including a special purpose parity computation engine.) 315 233.33 T
0 F
1.76 (Disk arrays inherently require extra computation) 336.6 223.33 P
1.85 (for error correction codes. Several popular disk array) 315 213.33 P
0.85 (architectures, such as RAID Levels 3 and 5, use parity) 315 203.33 P
0.07 (as the error correction code [Patterson88, Lee91]. Parity) 315 193.33 P
1.96 (is computed when writing data to the disk array and) 315 183.33 P
0.26 (when reconstructing the contents of a failed disk. Parity) 315 173.33 P
0.46 (computation is a simple but bandwidth intensive opera-) 315 163.33 P
0.41 (tion that workstations, with their limited memory band-) 315 153.33 P
0.71 (width, perform too slowly) 315 143.33 P
0.71 (. T) 420.86 143.33 P
0.71 (o address this concern, we) 431.97 143.33 P
3.39 (provide a simple parity computation engine on the) 315 133.33 P
0.19 (XBUS card to speed up disk writes and reconstructions.) 315 123.33 P
315 92 540 112 C
315 100 447 100 2 L
0.25 H
2 Z
0 X
0 K
N
0 0 612 792 C
0 10 Q
0 X
0 K
(2.  W) 315 85.33 T
(e have since routed the full 8x8 version of the) 336.13 85.33 T
(XBUS card.) 315 75.33 T
FMENDPAGE
%%EndPage: "4" 3
%%Page: "3" 3
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
0 10 Q
0.25 (connect that can support many dif) 72 713.33 P
0.25 (ferent RAID architec-) 208.81 713.33 P
1.99 (tures. In particular) 72 703.33 P
1.99 (, RAID-II supports RAID Level 5,) 149.12 703.33 P
3.87 (which supports many independent I/Os in parallel.) 72 693.33 P
0.13 (RAID Level 3, on the other hand, supports only one I/O) 72 683.33 P
(at a time.) 72 673.33 T
1.95 (In summary) 93.6 663.33 P
1.95 (, although most supercomputer mass) 142.92 663.33 P
-0.01 (storage systems can transfer data at rates up to 10 MB/s,) 72 653.33 P
0.32 (this is still insuf) 72 643.33 P
0.32 (\336cient to support diskless supercomput-) 136.37 643.33 P
0.3 (ers. Furthermore, they neglect the performance of small) 72 633.33 P
0.22 (NFS-type requests in order to optimize the performance) 72 623.33 P
2.14 (of high-bandwidth data transfers. Finally) 72 613.33 P
2.14 (, even if the) 243.4 613.33 P
0.49 (supercomputer mass storage systems were optimized to) 72 603.33 P
2.01 (support NFS-type requests, it would be economically) 72 593.33 P
0.23 (infeasible to use mainframes as \336le servers for worksta-) 72 583.33 P
0.02 (tions. The mass storage system closest to RAID-II is the) 72 573.33 P
3.36 (Los Alamos High-Performance Data System, which) 72 563.33 P
2.49 (uses a HIPPI-attached IBM disk array) 72 553.33 P
2.49 (, but this disk) 235.65 553.33 P
(array only supports one I/O at a time.) 72 543.33 T
2 12 Q
(2 RAID-II Ar) 72 516 T
(chitectur) 141.73 516 T
(e) 187.47 516 T
0 10 Q
3.18 (RAID-II is a high-performance \336le server that) 93.6 505.33 P
0.61 (interfaces a SCSI-based disk array to a HIPPI network.) 72 495.33 P
0.3 (Our goal in designing RAID-II is to provide high-band-) 72 485.33 P
2.29 (width access from the network to a lar) 72 475.33 P
2.29 (ge disk array) 240.79 475.33 P
0.14 (without transferring data through the relatively slow \336le) 72 465.33 P
72 72 504 459 C
170.6 295.03 462.86 295.03 2 L
1 H
2 Z
10 X
0 K
N
458.82 413.17 491.22 438.15 R
7 X
V
0.5 H
0 Z
0 X
N
0 10 Q
(LINK) 466.24 428.81 T
453.01 408.18 485.41 433.15 R
7 X
V
0 X
N
(LINK) 460.43 423.82 T
447.04 403.86 479.44 428.83 R
7 X
V
0 X
N
(LINK) 454.46 419.5 T
2 X
90 450 33.07 31.39 373.63 271.2 G
1 H
90 450 33.07 31.39 373.63 271.2 A
3 X
90 450 33.07 31.39 343.53 271.2 G
90 450 33.07 31.39 343.53 271.2 A
4 X
90 450 33.07 31.39 313.43 271.2 G
90 450 33.07 31.39 313.43 271.2 A
5 X
90 450 33.07 31.39 271.17 271.2 G
90 450 33.07 31.39 271.17 271.2 A
440.73 399.54 473.12 424.51 R
7 X
V
0.5 H
0 X
N
(LINK) 448.15 415.18 T
434.25 395.22 466.64 420.19 R
7 X
V
0 X
N
(LINK) 441.67 410.86 T
252.01 263.6 293.18 295.32 R
7 X
V
0 X
N
279.14 297.73 279.14 281.29 2 L
7 X
V
1 H
2 Z
0 X
N
0 12 Q
(8 Port Interleaved) 278.05 418.04 T
(Memory \050128 MByte\051) 267.71 408.32 T
273.78 352.43 368.27 391.57 R
0.5 H
0 Z
N
(8 x 8 x 32-bit) 289.11 374.17 T
(Crossbar) 300.11 364.45 T
279.18 401.7 279.18 391.57 2 L
2 Z
N
290.65 401.7 290.65 391.57 2 L
N
302.8 401.7 302.8 391.57 2 L
N
314.95 401.7 314.95 391.57 2 L
N
326.83 401.97 326.83 391.84 2 L
N
338.3 401.97 338.3 391.84 2 L
N
350.45 401.97 350.45 391.84 2 L
N
362.6 401.97 362.6 391.84 2 L
N
(VME) 267.7 324.76 T
265 321.92 291.33 334.07 R
0 Z
N
0 10 Q
(VME) 296.46 324.76 T
293.76 321.92 320.08 334.07 R
N
(VME) 326.15 324.76 T
323.45 321.92 349.78 334.07 R
N
(VME) 355.58 324.76 T
352.88 321.92 379.21 334.07 R
N
279.85 352.43 279.85 334.2 2 L
2 Z
N
360.17 352.43 360.17 334.2 2 L
N
310.23 352.43 310.23 334.2 2 L
N
331.15 352.43 331.15 334.2 2 L
N
0 12 Q
(VME) 390.68 356.08 T
387.98 353.24 414.31 365.39 R
0 Z
N
(XOR) 391.22 380.11 T
387.85 377.27 414.17 389.41 R
N
368.27 386.85 392.57 386.85 2 L
2 Z
N
368.27 362.55 392.57 362.55 2 L
N
254.2 386.85 273.78 386.85 2 L
N
253.53 365.92 273.78 365.92 2 L
N
253.53 379.42 262.3 379.42 2 L
10 X
N
262.3 379.42 262.3 342.3 2 L
11 X
N
262.3 357.83 253.53 357.83 2 L
10 X
N
268.38 342.3 268.38 334.2 2 L
11 X
N
299.43 342.3 299.43 334.2 2 L
N
341.95 342.3 341.95 334.07 2 L
N
373.67 342.17 373.67 334.07 2 L
N
384.2 380.24 384.2 342.17 2 L
N
384.2 380.64 392.71 380.64 2 L
10 X
N
384.2 355.13 392.3 355.13 2 L
N
228.96 315.57 420.65 429.64 R
7 X
V
0 Z
0 X
N
(4 Port Interleaved) 284.53 411.56 T
(Memory \05032 MB\051) 284.52 401.84 T
280.26 345.95 374.75 385.09 R
N
(4-by-8 by 32-bit) 288.6 367.69 T
(Crossbar) 306.58 357.97 T
285.66 395.22 285.66 385.09 2 L
2 Z
N
369.08 395.49 369.08 385.36 2 L
N
0 10 Q
(VME) 274.18 318.28 T
271.48 315.44 297.81 327.59 R
0 Z
N
(VME) 302.94 318.28 T
300.24 315.44 326.56 327.59 R
N
(VME) 332.63 318.28 T
329.93 315.44 356.26 327.59 R
N
(VME) 362.06 318.28 T
359.36 315.44 385.69 327.59 R
N
286.33 345.95 286.33 327.72 2 L
2 Z
N
366.65 345.95 366.65 327.72 2 L
N
316.7 345.95 316.7 327.72 2 L
N
337.63 345.95 337.63 327.72 2 L
N
(VME) 228.6 326.6 T
(XOR) 397.7 373.63 T
394.33 370.79 420.65 382.93 R
0 Z
N
374.75 380.37 394.02 380.37 2 L
2 Z
N
(HIPPIS) 230.86 372.95 T
228.96 370.11 262.71 382.26 R
0 Z
N
(HIPPD) 231.01 352.98 T
228.96 350.13 262.71 362.28 R
N
262.4 380.37 280.26 380.37 2 L
2 Z
N
262.4 359.45 280.26 359.45 2 L
N
262.4 372.95 268.78 372.95 2 L
10 X
N
268.78 372.95 268.78 335.82 2 L
11 X
N
268.78 335.82 385.92 335.82 2 L
10 X
N
268.78 351.35 262.4 351.35 2 L
N
274.86 335.82 274.86 327.72 2 L
11 X
N
305.91 335.82 305.91 327.72 2 L
N
348.43 335.82 348.43 327.59 2 L
N
380.15 335.69 380.15 327.59 2 L
N
385.96 373.76 385.96 335.69 2 L
N
385.96 374.16 394.46 374.16 2 L
10 X
N
2 13 Q
0 X
(XBUS) 234.49 417.76 T
(Card) 234.49 405.04 T
146.21 380.68 178.03 410.76 R
7 X
V
0 Z
0 X
N
0 10 Q
(HIPPIS) 148.23 392.88 T
154.42 360.89 188.27 390.98 R
7 X
V
0 X
N
(HIPPID) 156.45 373.09 T
208.4 414.49 208.4 325.4 2 L
7 X
V
3 H
2 Z
0 X
N
188.83 367.95 207.73 367.95 2 L
7 X
V
2 H
0 X
N
179.38 393.6 216.5 393.6 2 L
7 X
V
0 X
N
227.98 376.05 218.53 376.05 2 L
7 X
V
0 X
N
227.98 356.48 208.4 356.48 2 L
7 X
V
0 X
N
146.61 372.45 154.4 367.95 146.61 363.45 3 L
0 Z
N
127.4 367.95 153.4 367.95 2 L
7 X
V
2 Z
0 X
N
126.43 400.57 118.63 405.07 126.43 409.57 3 L
0 Z
N
146.3 405.07 119.63 405.07 2 L
7 X
V
2 Z
0 X
N
(HIPPI) 119.98 383.62 T
232.87 249.96 286.87 281.69 R
7 X
V
0.5 H
0 Z
0 X
N
0 12 Q
(VME) 245.93 272.39 T
(Disk) 247.93 262.66 T
(Controller) 234.61 252.95 T
251.3 249.97 251.3 228.23 2 L
7 X
V
2 Z
0 X
N
267.23 249.56 267.23 228.23 2 L
7 X
V
0 X
N
(Control) 385.28 328.95 T
(Bus) 394.27 319.23 T
139.55 444.9 483.11 444.9 2 L
7 X
V
0 X
N
439.92 444.9 439.92 409.1 2 L
7 X
V
0 X
N
(Ethernet) 213.94 446.39 T
1 F
(\050Contr) 257.57 446.39 T
(ol and Low Latency T) 289.11 446.39 T
(ransfers\051) 393.07 446.39 T
0 10 Q
(TMC) 152.38 401.84 T
(TMC) 160.88 381.86 T
280.26 395.22 373.4 424.24 R
0 Z
N
438.57 443.52 441.27 446.22 1.35 RR
V
N
1 12 Q
(High Bandwidth) 113.24 425.8 T
(T) 130.06 416.08 T
(ransfers) 136.07 416.08 T
217.85 409.1 217.85 320.67 2 L
V
3 H
2 Z
N
0 F
(LINK) 254.3 283.02 T
291.02 309.2 291.02 295.7 2 L
V
1 H
N
271.17 290.98 292.77 311.9 2 L
V
N
286.02 314.6 286.02 305.83 2 L
V
N
482.44 407.58 482.44 274.44 2 L
V
11 X
N
481.93 274.44 375.29 274.44 2 L
0 X
V
10 X
N
0 10 Q
0 X
(HIPPIS Bus) 176 315.17 T
(HIPPID Bus) 200.13 305.45 T
423.72 384.15 456.12 409.12 R
7 X
V
0.5 H
0 Z
0 X
N
0 12 Q
(Server) 424.8 388.75 T
(File) 431.11 398.47 T
489.19 412.64 489.19 267.86 2 L
7 X
V
1 H
2 Z
11 X
N
489.02 267.69 404.48 267.69 2 L
7 X
V
10 X
N
199.36 420.84 199.36 331.74 2 L
7 X
V
0 X
N
0 10 Q
(VME) 197.6 422.15 T
199.63 403.7 178.03 403.7 2 L
7 X
V
0 X
N
198.95 378.05 188.15 378.05 2 L
7 X
V
0 X
N
313.02 314.6 313.02 301.78 2 L
7 X
V
0 X
N
342.72 314.6 342.72 301.78 2 L
7 X
V
0 X
N
372.42 314.6 372.42 303.13 2 L
7 X
V
0 X
N
475.76 404.03 475.76 280.51 2 L
7 X
V
11 X
N
475.86 280.01 345.76 280.01 2 L
7 X
V
10 X
N
469.28 399.48 469.28 286.93 2 L
7 X
V
11 X
N
468.77 286.93 292.6 286.93 2 L
7 X
V
10 X
N
0 X
(Four VME Disk Controllers) 289.23 231.47 T
154.71 333.56 187.11 358.53 R
7 X
V
0.5 H
0 Z
0 X
N
(LINK) 160.61 343.13 T
187.31 346.32 198.95 346.32 2 L
7 X
V
1 H
2 Z
0 X
N
170.6 333.5 170.6 295.03 2 L
7 X
V
11 X
N
462.86 394.92 462.86 295.03 2 L
7 X
V
11 X
N
0 X
(VME Ribbon) 413.41 259.49 T
(Cable Segments) 408.14 251.39 T
(Control Paths) 413.13 243.29 T
72 78 500.4 150 R
7 X
V
2 11 Q
0 X
0.78 (Figur) 72 143.33 P
0.78 (e 1: RAID-II Organization) 98.05 143.33 P
0 10 Q
0.71 (. A high-bandwidth crossbar interconnect ties the network interface) 225.2 143.33 P
0.46 (\050HIPPI\051, the disk controllers, a multiported memory system, and a parity computation engine. An internal) 72 133.33 P
0.36 (control bus provides access to the crossbar ports, while external point-to-point VME links provide control) 72 123.33 P
-0.08 (paths to the surrounding SCSI and HIPPI interface boards. Up to two VME disk controllers can be attached) 72 113.33 P
0.78 (to each of the four VME interfaces. The design originally had 8 memory ports and 128 MB of memory;) 72 103.33 P
(however) 72 93.33 T
(, we built a four memory port version to reduce manufacturing time.) 106.01 93.33 T
228.6 324 255.6 342 R
0.5 H
0 Z
N
198 331.2 230.4 331.2 2 L
2 Z
N
270 336.17 255.6 336.17 2 L
10 X
N
312.1 395.22 312.1 385.09 2 L
0 X
N
340.33 395.22 340.33 385.09 2 L
N
221.6 237.14 275.6 268.86 R
7 X
V
0 Z
0 X
N
0 12 Q
(VME) 234.67 259.56 T
(Disk) 236.66 249.84 T
(Controller) 223.34 240.12 T
240.03 237.14 240.03 165.6 2 L
7 X
V
2 Z
0 X
N
7 X
90 450 5.4 3.6 232.2 230.4 G
0 Z
0 X
90 450 5.4 3.6 232.2 230.4 A
227.2 229.4 227.2 218.6 2 L
7 X
V
2 Z
0 X
N
238.2 229.4 238.2 218.6 2 L
7 X
V
0 X
N
7 X
90 450 5.4 3.6 232.2 219.4 G
0 Z
0 X
90 450 5.4 3.6 232.2 219.4 A
7 X
90 450 5.4 3.6 232.2 206.6 G
0 X
90 450 5.4 3.6 232.2 206.6 A
227.2 205.6 227.2 194.8 2 L
7 X
V
2 Z
0 X
N
238.2 205.6 238.2 194.8 2 L
7 X
V
0 X
N
7 X
90 450 5.4 3.6 232.2 195.6 G
0 Z
0 X
90 450 5.4 3.6 232.2 195.6 A
7 X
90 450 5.4 3.6 232.2 183.8 G
0 X
90 450 5.4 3.6 232.2 183.8 A
227.2 182.8 227.2 172 2 L
7 X
V
2 Z
0 X
N
238.2 182.8 238.2 172 2 L
7 X
V
0 X
N
7 X
90 450 5.4 3.6 232.2 172.8 G
0 Z
0 X
90 450 5.4 3.6 232.2 172.8 A
259.2 237.14 259.2 165.6 2 L
7 X
V
2 Z
0 X
N
7 X
90 450 5.4 3.6 252.6 230.4 G
0 Z
0 X
90 450 5.4 3.6 252.6 230.4 A
246.6 229.4 246.6 218.6 2 L
7 X
V
2 Z
0 X
N
257.6 229.4 257.6 218.6 2 L
7 X
V
0 X
N
7 X
90 450 5.4 3.6 252.6 219.4 G
0 Z
0 X
90 450 5.4 3.6 252.6 219.4 A
7 X
90 450 5.4 3.6 252.6 206.6 G
0 X
90 450 5.4 3.6 252.6 206.6 A
246.6 205.6 246.6 194.8 2 L
7 X
V
2 Z
0 X
N
257.6 205.6 257.6 194.8 2 L
7 X
V
0 X
N
7 X
90 450 5.4 3.6 252.6 195.6 G
0 Z
0 X
90 450 5.4 3.6 252.6 195.6 A
7 X
90 450 5.4 3.6 252.6 183.8 G
0 X
90 450 5.4 3.6 252.6 183.8 A
246.6 182.8 246.6 172 2 L
7 X
V
2 Z
0 X
N
257.6 182.8 257.6 172 2 L
7 X
V
0 X
N
7 X
90 450 5.4 3.6 252.6 172.8 G
0 Z
0 X
90 450 5.4 3.6 252.6 172.8 A
0 0 612 792 C
0 10 Q
0 X
0 K
0.88 (server \050a Sun4/280 workstation\051 backplane. T) 315 713.33 P
0.88 (o do this,) 501.32 713.33 P
1.57 (we designed a custom printed-circuit board called the) 315 703.33 P
1 F
0.91 (XBUS car) 315 693.33 P
0.91 (d) 355.79 693.33 P
0 F
0.91 (. The main purpose of the XBUS card is to) 360.79 693.33 P
1.01 (provide a high-bandwidth path between the major sys-) 315 683.33 P
0.58 (tem components: the HIPPI network, four VME busses) 315 673.33 P
2.69 (that connect to VME disk controllers, and an inter-) 315 663.33 P
0.22 (leaved, multiported semiconductor memory) 315 653.33 P
0.22 (. The XBUS) 489.88 653.33 P
0.16 (card also contains a parity computation engine that gen-) 315 643.33 P
1.48 (erates parity for writes and reconstruction on the disk) 315 633.33 P
-0.03 (array) 315 623.33 P
-0.03 (. The entire system is controlled by an external Sun) 334.88 623.33 P
-0.09 (4/280 \336le server through a memory-mapped control reg-) 315 613.33 P
0.94 (ister interface. Figure 1 shows a block diagram for the) 315 603.33 P
0.34 (controller) 315 593.33 P
0.34 (. T) 353.31 593.33 P
0.34 (o minimize the design ef) 364.05 593.33 P
0.34 (fort, we used com-) 464.05 593.33 P
4.69 (mercially available components whenever possible.) 315 583.33 P
0.44 (Thinking Machines \050TMC\051 provided a board set for the) 315 573.33 P
2.09 (HIPPI network interface; Interphase Corporation pro-) 315 563.33 P
0.76 (vided VME-based, dual SCSI, Cougar disk controllers;) 315 553.33 P
1.41 (Sun Microsystems provided the Sun 4/280 \336le server;) 315 543.33 P
(and IBM donated disk drives and DRAM.) 315 533.33 T
2 12 Q
(2.1  XBUS Card Ar) 315 516 T
(chitectur) 414.39 516 T
(e) 460.13 516 T
0 10 Q
1.02 (The XBUS card implements a 4x8 \050four memory) 336.6 505.33 P
-0.07 (ports and eight client ports\051, 32-bit wide crossbar) 315 495.33 P
-0.07 (, which) 510.64 495.33 P
0.94 (we call the XBUS. All XBUS transfers involve one of) 315 485.33 P
0.57 (the four memory ports as either the source or the desti-) 315 475.33 P
0.57 (nation of the transfer) 315 465.33 P
0.57 (. Each memory port is designed to) 399.71 465.33 P
FMENDPAGE
%%EndPage: "3" 2
%%Page: "2" 2
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
0 10 Q
1.17 (Sun 4/280 goes through the CPU\325) 72 713.33 P
1.17 (s virtually addressed) 212.5 713.33 P
0.09 (cache, data transfers experience interference from cache) 72 703.33 P
0.66 (\337ushes. Finally) 72 693.33 P
0.66 (, high-bandwidth performance is limited) 133.09 693.33 P
0.85 (by the low bandwidth of the Sun 4/280\325) 72 683.33 P
0.85 (s VME system) 236.45 683.33 P
2.79 (bus. Although nominally rated at 40 MB/s, the bus) 72 673.33 P
(becomes saturated at 9 MB/s.) 72 663.33 T
1.43 (The problems RAID-I experienced are typical of) 93.6 653.33 P
0.09 (many \324) 72 643.33 P
0.09 (\324CPU-centric\325) 99.39 643.33 P
0.09 (\325 workstations that are designed for) 155.26 643.33 P
-0.09 (good processor performance but fail to support adequate) 72 633.33 P
0.78 (I/O bandwidth. In such systems, the memory system is) 72 623.33 P
1.53 (designed so that the CPU has the fastest and highest-) 72 613.33 P
1.78 (bandwidth path to memory) 72 603.33 P
1.78 (. For busses or backplanes) 184.68 603.33 P
0.68 (farther away from the CPU, the available bandwidth to) 72 593.33 P
2.5 (memory drops quickly) 72 583.33 P
2.5 (. Our experience with RAID-I) 166.84 583.33 P
0.81 (indicates that the memory systems of workstations are,) 72 573.33 P
0.44 (in general, poorly suited for supporting high-bandwidth) 72 563.33 P
(I/O.) 72 553.33 T
2 12 Q
(1.2.2  Auspex NS5000) 72 536 T
0 10 Q
1.79 (The Auspex NS5000 [Nelson90] is designed for) 93.6 525.33 P
0.79 (high-performance, NFS \336le service [Sandber) 72 515.33 P
0.79 (g85]. NFS) 254.56 515.33 P
1.68 (is the most common network \336le system protocol for) 72 505.33 P
0.03 (workstation-based computing environments. It is prima-) 72 495.33 P
4.66 (rily designed to support operations on small and) 72 485.33 P
0.05 (medium sized \336les. Because NFS transfers \336les in small) 72 475.33 P
(individual packets, it is inef) 72 465.33 T
(\336cient for lar) 182.59 465.33 T
(ge \336les.) 234.59 465.33 T
1.34 (In NFS, as with most network \336le systems, each) 93.6 455.33 P
1.41 (packet requires a relatively constant CPU overhead to) 72 445.33 P
0.86 (process device interrupts and manage the network pro-) 72 435.33 P
3.67 (tocol. Because of this per) 72 425.33 P
3.67 (-packet overhead, current) 187.81 425.33 P
0.52 (workstations lack suf) 72 415.33 P
0.52 (\336cient processing power to handle) 157.8 415.33 P
0.56 (the lar) 72 405.33 P
0.56 (ge numbers of small NFS packets that are gener-) 97.64 405.33 P
2.18 (ated at high data transfer rates. Additionally) 72 395.33 P
2.18 (, NFS is) 260.14 395.33 P
3.07 (designed to support many clients making small \336le) 72 385.33 P
1.53 (requests independent of each other) 72 375.33 P
1.53 (. Although an indi-) 216.34 375.33 P
1.3 (vidual client\325) 72 365.33 P
1.3 (s request stream may exhibit locality) 125.77 365.33 P
1.3 (, the) 278.49 365.33 P
2.24 (sequence of requests seen by the server will include) 72 355.33 P
1.49 (interleaved requests from many clients and thus show) 72 345.33 P
0.32 (less locality) 72 335.33 P
0.32 (. Finally) 119.15 335.33 P
0.32 (, NFS was designed for workstation) 152.14 335.33 P
0.72 (clients that typically handle small \336les. As a result, the) 72 325.33 P
0.72 (protocols are optimized for \336les that can be transferred) 72 315.33 P
(in relatively few packets.) 72 305.33 T
1.1 (The NS5000 handles the network processing, \336le) 93.6 295.33 P
2.48 (system management, and disk control using separate) 72 285.33 P
1.93 (dedicated processors. This) 72 275.33 P
1 F
1.93 (functional multipr) 186.05 275.33 P
1.93 (ocessing) 260.08 275.33 P
0 F
1.93 (,) 294.5 275.33 P
1.92 (in contrast to symmetric multiprocessing, makes syn-) 72 265.33 P
0.95 (chronization between processes explicit and allows the) 72 255.33 P
-0.01 (performance of the \336le server to scale by adding proces-) 72 245.33 P
1.43 (sors, network attachments and disks. T) 72 235.33 P
1.43 (ypical NFS \336le) 233.62 235.33 P
1.47 (servers, on the other hand, perform all functions on a) 72 225.33 P
1.25 (single processor) 72 215.33 P
1.25 (. In such systems, performance can be) 137.38 215.33 P
0.54 (scaled to only a very limited degree by adding network) 72 205.33 P
4.93 (attachments and disks because the processor will) 72 195.33 P
2.64 (quickly become a bottleneck. Due to this functional) 72 185.33 P
-0.16 (multiprocessing, the NS5000 can support up to 770 NFS) 72 175.33 P
1.9 (I/Os per second with 8 Ethernets and 16 disks [Hor-) 72 165.33 P
(ton90].) 72 155.33 T
0.7 (Although the NS5000 is good at supporting small) 93.6 145.33 P
2.95 (low-latency NFS requests, it is unsuitable for high-) 72 135.33 P
1.79 (bandwidth applications. The use of a single 55 MB/s) 72 125.33 P
1.3 (VME bus to connect the networks, disks and memory) 72 115.33 P
0.81 (limits the aggregate I/O bandwidth of the system. NFS) 72 105.33 P
0.95 (is also very inef) 72 95.33 P
0.95 (\336cient for lar) 138.24 95.33 P
0.95 (ge \336les because it always) 192.15 95.33 P
-0.2 (breaks up \336les into small packets which are sent individ-) 72 85.33 P
0.47 (ually over the network. This results in fragmentation of) 72 75.33 P
0.78 (the available network bandwidth and forces the receiv-) 315 713.33 P
(ing system to handle a lar) 315 703.33 T
(ge number of interrupts.) 417.25 703.33 T
2 12 Q
-0.19 (1.2.3  Super) 315 686 P
-0.19 (computer Mass Storage Systems) 375.04 686 P
0 10 Q
2.32 (Almost all supercomputer mass storage systems) 336.6 675.33 P
0.61 (use a mainframe as a high-performance \336le server) 315 665.33 P
0.61 (. The) 518.85 665.33 P
2.17 (mainframe runs the \336le system and provides a high-) 315 655.33 P
0.3 (bandwidth data path between its channel-based I/O sys-) 315 645.33 P
1.14 (tem and supercomputer clients via a high-performance) 315 635.33 P
1.77 (channel or network interface. There are several prob-) 315 625.33 P
1 (lems with today\325) 315 615.33 P
1 (s supercomputer mass storage system.) 383.62 615.33 P
2.94 (First, most supercomputer mass storage systems are) 315 605.33 P
2.34 (designed primarily for capacity) 315 595.33 P
2.34 (, so very few support) 446.55 595.33 P
3.16 (data transfer rates over 10 MB/s. For performance,) 315 585.33 P
0.03 (supercomputer applications rely on locally attached par-) 315 575.33 P
1.75 (allel-transfer disks. Second, supercomputer mass stor-) 315 565.33 P
0.63 (age systems are not designed to service a lar) 315 555.33 P
0.63 (ge number) 496.91 555.33 P
-0.2 (of small \336le requests and are rarely used as primary stor-) 315 545.33 P
1.71 (age systems for lar) 315 535.33 P
1.71 (ge numbers of client workstations.) 395.18 535.33 P
0.67 (Third, mainframes are very expensive, costing millions) 315 525.33 P
1.01 (of dollars. The following brie\337y describes the MSS-II,) 315 515.33 P
0.29 (NCAR, LSS, and Los Alamos National Labs mass stor-) 315 505.33 P
(age systems) 315 495.33 T
1.49 (MSS-II [T) 336.6 485.33 P
1.49 (weten90], the NASA Ames mass stor-) 379.31 485.33 P
0.18 (age system, uses an Amdahl 5880 as a \336le server) 315 475.33 P
0.18 (. MSS-) 511.49 475.33 P
0.24 (II achieves data transfer rates up to 10 MB/s by striping) 315 465.33 P
0.3 (data over multiple disks and transferring data over mul-) 315 455.33 P
2.4 (tiple network channels. The practice of striping data) 315 445.33 P
2.03 (across disks to provide higher data transfer rates has) 315 435.33 P
2.68 (been used for some time in supercomputer environ-) 315 425.33 P
(ments.) 315 415.33 T
-0.13 (The mass storage system at NCAR [Nelson87], the) 336.6 405.33 P
2.29 (National Center for Atmospheric Research, is imple-) 315 395.33 P
2.8 (mented using Hyperchannel and an IBM mainframe) 315 385.33 P
3.67 (running MVS. The NCAR mass storage system is) 315 375.33 P
2.02 (unique in that it provides a direct data path between) 315 365.33 P
3.87 (supercomputers and the IBM mainframe\325) 315 355.33 P
3.87 (s channel-) 495.34 355.33 P
2.52 (based storage controllers. On a \336le access, data can) 315 345.33 P
4.93 (bypass the mainframe and be transferred directly) 315 335.33 P
(between the storage devices and the supercomputers.) 315 325.33 T
2.7 (The Lawrence Livermore National Laboratory\325) 336.6 315.33 P
2.7 (s) 536.11 315.33 P
0.65 (LINCS Storage System \050LSS\051 [Foglesong90], is one of) 315 305.33 P
3.71 (the systems upon which the Mass Storage System) 315 295.33 P
0.26 (\050MSS\051 Reference Model [Coleman90] is based. A nota-) 315 285.33 P
0.63 (ble aspect of LSS is that control and data messages are) 315 275.33 P
0.83 (always transmitted independently) 315 265.33 P
0.83 (. This allows the con-) 450.36 265.33 P
1.43 (trol and data messages to take dif) 315 255.33 P
1.43 (ferent paths through) 456.64 255.33 P
-0.05 (the system. For example, a control message requesting a) 315 245.33 P
0.74 (write might be sent to the bit\336le server via an Ethernet) 315 235.33 P
0.85 (but the data itself would be sent directly to the storage) 315 225.33 P
1.58 (server via a high speed HIPPI channel, bypassing the) 315 215.33 P
(bit\336le server) 315 205.33 T
(.) 364.69 205.33 T
3.11 (Los Alamos National Labs\325) 336.6 195.33 P
3.11 (s High-Performance) 456.13 195.33 P
1.6 (Data System [Collins91] is an experimental prototype) 315 185.33 P
1.01 (designed to support high-bandwidth I/O for the LANL) 315 175.33 P
0.47 (supercomputers. The LANL design is quite close to the) 315 165.33 P
-0.13 (RAID-II architecture. It directly connects an IBM RAID) 315 155.33 P
0.63 (Level 3 disk array to a HIPPI network and controls the) 315 145.33 P
0.02 (data movement remotely \050over an Ethernet\051 from a IBM) 315 135.33 P
2.29 (RISC/6000. Los Alamos has demonstrated data rates) 315 125.33 P
0.89 (close to the maximum data rate of the IBM disk array) 315 115.33 P
0.89 (,) 537.5 115.33 P
-0.09 (which is close to 60 MB/s. The main dif) 315 105.33 P
-0.09 (ference between) 474.88 105.33 P
-0.1 (LANL) 315 95.33 P
-0.1 (\325) 340.73 95.33 P
-0.1 (s High-Performance Data System and RAID-II is) 343.5 95.33 P
1.77 (that LANL uses a bit-striped, or RAID Level 3, disk) 315 85.33 P
1.27 (array) 315 75.33 P
1.27 (, whereas RAID-II uses a \337exible, crossbar inter-) 334.88 75.33 P
FMENDPAGE
%%EndPage: "2" 1
%%Page: "1" 1
612 792 0 FMBEGINPAGE
72 54 540 54 2 L
0.25 H
2 Z
0 X
0 K
N
0 8 Q
(Performance and Design Evaluation of the RAID-II Storage Server) 216.53 42.62 T
2 14 Q
(Performance and Design Evaluation) 76.25 710.67 T
(of the RAID-II Storage Server) 93.96 690.67 T
0 10 Q
(Peter M. Chen, Edward K. Lee, Ann L. Drapeau, Ken) 76.95 667.33 T
(Lutz, Ethan L. Miller) 75.14 657.33 T
(, Srinivasan Seshan, Ken Shirrif) 159.96 657.33 T
(f,) 288.03 657.33 T
(David A. Patterson, Randy H. Katz) 113.99 647.33 T
2 11 Q
4.74 (Abstract:) 72 625.33 P
1 10 Q
4.31 (RAID-II is a high-bandwidth, network-) 124.02 625.33 P
1.88 (attached storage server designed and implemented at) 72 615.33 P
1.2 (the University of California at Berkeley) 72 605.33 P
1.2 (. In this paper) 235.94 605.33 P
1.2 (,) 294.5 605.33 P
1.47 (we measur) 72 595.33 P
1.47 (e the performance of RAID-II and evaluate) 116.12 595.33 P
0.73 (various ar) 72 585.33 P
0.73 (chitectural decisions made during the design) 113.73 585.33 P
-0.06 (pr) 72 575.33 P
-0.06 (ocess. W) 80.51 575.33 P
-0.06 (e \336rst measur) 114.51 575.33 P
-0.06 (e the end-to-end performance of) 168.44 575.33 P
1.38 (the system to be appr) 72 565.33 P
1.38 (oximately 20 MB/s for both disk) 162.1 565.33 P
1.46 (array r) 72 555.33 P
1.46 (eads and writes. W) 101.69 555.33 P
1.46 (e then perform a bottleneck) 181.23 555.33 P
0.38 (analysis by examining the performance of each individ-) 72 545.33 P
0.02 (ual subsystem and conclude that the disk subsystem lim-) 72 535.33 P
-0.03 (its performance. By adding a custom inter) 72 525.33 P
-0.03 (connect boar) 239.94 525.33 P
-0.03 (d) 292 525.33 P
1.54 (with a high-speed memory and bus system and parity) 72 515.33 P
0.94 (engine, we ar) 72 505.33 P
0.94 (e able to achieve a performance speedup) 127.63 505.33 P
0.18 (of 8 to 15 over a comparative system using only off-the-) 72 495.33 P
(shelf har) 72 485.33 T
(dwar) 106.89 485.33 T
(e.) 127.07 485.33 T
2 12 Q
(1 Intr) 72 468 T
(oduction) 101.43 468 T
0 10 Q
1.19 (RAID-II is a high-bandwidth, network \336le server) 93.6 457.33 P
0.54 (designed and implemented at the University of Califor-) 72 447.33 P
0.18 (nia at Berkeley as part of a project to study high-perfor-) 72 437.33 P
1.57 (mance, lar) 72 427.33 P
1.57 (ge-capacity) 115.02 427.33 P
1.57 (, highly-reliable storage systems.) 160.43 427.33 P
1.51 (RAID-II is designed for the heterogeneous computing) 72 417.33 P
0.1 (environments of the future, consisting of diskless super-) 72 407.33 P
-0.24 (computers, visualization workstations, multi-media plat-) 72 397.33 P
(forms, and UNIX workstations.) 72 387.33 T
0.43 (Other papers [Lee92, Katz93] discuss in detail the) 93.6 377.33 P
1.28 (architecture and implementation of RAID-II. The goal) 72 367.33 P
-0.1 (of this paper is to evaluate the decisions made in design-) 72 357.33 P
0.22 (ing RAID-II. W) 72 347.33 P
0.22 (e evaluate those decisions by measuring) 135.75 347.33 P
2.21 (the performance of RAID-II and its components and) 72 337.33 P
0.19 (comparing it to the performance of RAID-I, a prototype) 72 327.33 P
1.73 (network \336le server assembled at U.C. Berkeley using) 72 317.33 P
3.39 (of) 72 307.33 P
3.39 (f-the-shelf parts. W) 80.15 307.33 P
3.39 (e are particularly interested in) 164.11 307.33 P
1.19 (evaluating the novel architectural features of RAID-II,) 72 297.33 P
2.79 (which are the crossbar) 72 287.33 P
2.79 (-based interconnect, the high-) 169.82 287.33 P
0.91 (speed data path between the network and the disk sys-) 72 277.33 P
1.09 (tem, the separate network path for small and lar) 72 267.33 P
1.09 (ge \336le) 271.21 267.33 P
0.89 (accesses, and the exclusive-) 72 257.33 P
0.89 (or unit for fast parity com-) 186.24 257.33 P
(putation.) 72 247.33 T
0.41 (The rest of the paper is or) 93.6 237.33 P
0.41 (ganized as follows. Sec-) 198.61 237.33 P
0.85 (tion 1 provides motivation and reviews previous work;) 72 227.33 P
2.34 (Section 2 describes RAID-II\325) 72 217.33 P
2.34 (s architecture and high-) 195.88 217.33 P
1.75 (lights its novel features; Section 3 reports the perfor-) 72 207.33 P
1.34 (mance of RAID-II at the component and system level) 72 197.33 P
(and draws conclusions about our design decisions.) 72 187.33 T
2 12 Q
(1.1  Motivation) 72 170 T
0 10 Q
-0.09 (The development of RAID-II is motivated by three) 93.6 159.33 P
0.64 (key observations. First, we notice a trend toward band-) 72 149.33 P
0.91 (width-intensive applications: multi-media, CAD, lar) 72 139.33 P
0.91 (ge-) 284.24 139.33 P
2.06 (object databases, and scienti\336c visualization. Even in) 72 129.33 P
3.45 (well established application areas such as scienti\336c) 72 119.33 P
0.97 (computing, reductions in the cost of secondary storage) 72 109.33 P
3.64 (and the introduction of faster supercomputers have) 72 99.33 P
0.82 (caused a rapid growth in the size of datasets, requiring) 72 89.33 P
0.51 (faster I/O systems to transfer the increasing amounts of) 315 713.33 P
(data.) 315 703.33 T
2.53 (The second observation is that most of today\325) 336.6 693.33 P
2.53 (s) 536.11 693.33 P
0.58 (workstation-based \336le servers are incapable of support-) 315 683.33 P
-0.22 (ing high-bandwidth I/O. This was demonstrated in expe-) 315 673.33 P
1.19 (rience with our \336rst prototype, RAID-I. Moreover) 315 663.33 P
1.19 (, the) 521.6 663.33 P
0.94 (future I/O performance of server workstations is likely) 315 653.33 P
1.34 (to degrade relative to the overall performance of their) 315 643.33 P
1.01 (client workstations even if applications do not become) 315 633.33 P
2.39 (more I/O-intensive. This is because today\325) 315 623.33 P
2.39 (s worksta-) 496.26 623.33 P
2.98 (tions achieve high performance by using lar) 315 613.33 P
2.98 (ge, fast) 508.15 613.33 P
0.31 (caches without signi\336cantly improving the performance) 315 603.33 P
0.82 (of the primary memory and I/O systems. This problem) 315 593.33 P
0.77 (is mentioned by Hennessy and Jouppi [Hennessy91] in) 315 583.33 P
1.79 (their paper discussing how interactions between tech-) 315 573.33 P
3.39 (nology and computer architecture af) 315 563.33 P
3.39 (fect the perfor-) 473.27 563.33 P
(mance of computer systems.) 315 553.33 T
1.82 (Third, recent technological developments in net-) 336.6 543.33 P
1.09 (works and secondary storage systems make it possible) 315 533.33 P
1.37 (to build high-bandwidth, supercomputer \336le servers at) 315 523.33 P
2.26 (workstation prices. Until recently) 315 513.33 P
2.26 (, anyone wishing to) 454.93 513.33 P
0.72 (build a high-bandwidth, supercomputer I/O system had) 315 503.33 P
1.38 (to invest millions of dollars in proprietary) 315 493.33 P
1.38 (, high-band-) 489.77 493.33 P
0.42 (width network technology and expensive parallel-trans-) 315 483.33 P
5.49 (fer disks. But with the standardization of high-) 315 473.33 P
0.82 (performance interconnects and network, such as HIPPI) 315 463.33 P
-0.05 (and FDDI, and the commercialization of the disk arrays,) 315 453.33 P
2.53 (high-bandwidth networks and secondary storage sys-) 315 443.33 P
-0.15 (tems have suddenly become af) 315 433.33 P
-0.15 (fordable. What is lacking,) 436.91 433.33 P
0.2 (and the point that RAID-II addresses, is a storage archi-) 315 423.33 P
(tecture that can exploit these developments.) 315 413.33 T
2 12 Q
(1.2  Related W) 315 396 T
(ork) 389.62 396 T
0 10 Q
-0 (There are currently many existing \336le server archi-) 336.6 385.33 P
0.5 (tectures. W) 315 375.33 P
0.5 (e examine a few of them to serve as a back-) 360.21 375.33 P
-0.08 (ground for the discussion of various aspects of RAID-II.) 315 365.33 P
3.68 (First we examine RAID-I, a workstation-based \336le) 315 355.33 P
-0.11 (server with of) 315 345.33 P
-0.11 (f-the-shelf disk controllers and disks. Next) 370.11 345.33 P
1.11 (we look at the Auspex NS5000 \336le server) 315 335.33 P
1.11 (, which pro-) 489.2 335.33 P
5.14 (vides scalable high-performance NFS \336le service.) 315 325.33 P
4.25 (Finally) 315 315.33 P
4.25 (, we examine several mass storage systems) 342.67 315.33 P
2.43 (\050MSS\051 currently used by supercomputing centers for) 315 305.33 P
(high-capacity) 315 295.33 T
(, shared storage.) 368.75 295.33 T
2 12 Q
(1.2.1  RAID-I) 315 278 T
0 10 Q
0.82 ( W) 336.6 267.33 P
0.82 (e constructed RAID-I to see how well a work-) 348.56 267.33 P
-0.12 (station-based \336le server could provide access to the high) 315 257.33 P
0.7 (data and I/O rates supported by disk arrays. The proto-) 315 247.33 P
2.44 (type was constructed using a Sun 4/280 workstation) 315 237.33 P
0.38 (with 128 MB of memory) 315 227.33 P
0.38 (, 28 5-1/4 inch SCSI disks and) 415.82 227.33 P
(four dual-string SCSI controllers.) 315 217.33 T
1.07 (Experiments with RAID-I show that it is good at) 336.6 207.33 P
4.25 (sustaining small random I/Os, performing approxi-) 315 197.33 P
2.52 (mately 300 4 KB random I/Os per second [Cherve-) 315 187.33 P
7.17 (nak91]. However) 315 177.33 P
7.17 (, RAID-I has proven woefully) 391.15 177.33 P
2.01 (inadequate for high-bandwidth I/O, sustaining at best) 315 167.33 P
-0.05 (2.3 MB/s to a user) 315 157.33 P
-0.05 (-level application on RAID-I. In com-) 388.16 157.33 P
0.63 (parison, a single disk on RAID-I can sustain 1.3 MB/s.) 315 147.33 P
1.13 (There are several reasons why RAID-I is ill-suited for) 315 137.33 P
1.84 (high-bandwidth I/O. The most serious is the memory) 315 127.33 P
0.86 (contention experienced on the Sun 4/280 server during) 315 117.33 P
0.42 (I/O operations. The copy operations performed in mov-) 315 107.33 P
0.09 (ing data between the kernel DMA buf) 315 97.33 P
0.09 (fers and buf) 466.92 97.33 P
0.09 (fers in) 514.65 97.33 P
0.51 (user space saturate the memory system when I/O band-) 315 87.33 P
0.39 (width reaches 2.3 MB/s. Second, because all I/O on the) 315 77.33 P
1 12 Q
(Appeared in in International Parallel Pr) 114 751.5 T
(ocessing Symposium 1993 W) 295.8 751.5 T
(orkshop on I/O) 433.6 751.5 T
FMENDPAGE
%%EndPage: "1" 0
%%Trailer
%%BoundingBox: 0 0 612 792
%%Pages: 11 -1
%%DocumentFonts: Times-Roman
%%+ Times-Italic
%%+ Times-Bold
%%+ Courier-Bold
